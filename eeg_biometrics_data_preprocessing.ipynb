{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg_biometrics.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkWW2ddS9Tqb"
      },
      "source": [
        "# EEG Biometrics\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCJoISV69Ugz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29a274b-2e77-4cea-b00d-6782d8051b6d"
      },
      "source": [
        "# Run this cell to load required libraries and mount your Drive folder\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "import json\r\n",
        "import csv\r\n",
        "\r\n",
        "\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjElrnbemlwr"
      },
      "source": [
        "# Parse the dataset and generate a csv containing the a subset of a specific task and selected channels\r\n",
        "def generate_csv(destination_path, file_name, labels, subset_indexes, folder_list, task_duration, task):\r\n",
        "  # Open CSV file for write and initialize the header\r\n",
        "  with open('/content/drive/MyDrive/eeg_dataset_right_hand_task_subset_9channels.csv', \"w\") as csv_file:\r\n",
        "    for label in labels:\r\n",
        "      csv_file.write(label + \",\")\r\n",
        "    csv_file.write(\"\\n\")\r\n",
        "    generate_csv_structure(csv_file, subset_indexes, folder_list, task_duration, task)\r\n",
        "\r\n",
        "# Navigate through the structure of the dataset and initialize variable for the subset\r\n",
        "def generate_csv_structure(csv_file, subset_indexes, folder_list, task_duration, task):\r\n",
        "  # Navigate through participants\r\n",
        "  for participant_folder in folder_list:\r\n",
        "\r\n",
        "    # Navigate through run folders for each participant\r\n",
        "    for folder in os.listdir(dataset_path + participant_folder):\r\n",
        "      date = folder.split(\"_\")[1]\r\n",
        "\r\n",
        "      # Navigate through the .npz recording file for each run\r\n",
        "      run_identifier = 1\r\n",
        "      for folder_run in os.listdir(dataset_path + participant_folder + \"/\" + folder):    \r\n",
        "        folder_run_name = folder_run.split('.')[0]\r\n",
        "        npz_data = np.load(dataset_path + participant_folder + \"/\" + folder+ \"/\" + folder_run)\r\n",
        "        npz_data_dict = dict(npz_data)\r\n",
        "        sample_rate = npz_data_dict['SampleRate'][0]\r\n",
        "        marked_tasks = npz_data_dict['MarkOnSignal'] \r\n",
        "        data = npz_data_dict['signal']\r\n",
        "\r\n",
        "        # We navigate throuhg marked tasks to find the index of the task we want to copy\r\n",
        "        # then write an entire line on file for each of the subsequent 2500 rows\r\n",
        "        trial = 1\r\n",
        "        for task in marked_tasks:\r\n",
        "          if task[1] == right_hand_task: \r\n",
        "            write_lines(csv_file, data, participant_folder, folder_run, date, run_identifier, trial, task_duration, sample_rate, subset_indexes, task)\r\n",
        "            trial += 1\r\n",
        "        run_identifier += 1 \r\n",
        "        \r\n",
        "        print(\"Run \" + folder_run + \" for participant \" + participant_folder + \" done!\") \r\n",
        "    print(\"Participant \" + participant_folder + \" done!\") \r\n",
        "\r\n",
        "# write all the 2500 lines starting from start_index\r\n",
        "def write_lines(csv_file, data, participant_folder, folder_run, date, run_identifier, trial, task_duration, sample_rate, subset_indexes, task):\r\n",
        "  start_index = task[0]\r\n",
        "  end_index = start_index + (sample_rate * task_duration) # copy all the 2500 lines starting from start_index\r\n",
        "  for trial_index in range(start_index, end_index):  \r\n",
        "    csv_file.write(participant_folder + \",\")\r\n",
        "    csv_file.write(date + \",\")\r\n",
        "    csv_file.write(\"Run\"+ str(run_identifier) + \",\")     \r\n",
        "    csv_file.write(str(task[1]) + \",\")   \r\n",
        "    csv_file.write(str(trial) + \",\")\r\n",
        "    try:\r\n",
        "      for index in subset_indexes:\r\n",
        "        csv_file.write(str(data[trial_index][index]))\r\n",
        "        if index != subset_indexes.size-1:\r\n",
        "          csv_file.write(',')\r\n",
        "      csv_file.write('\\n')\r\n",
        "    except:\r\n",
        "      print(\"Exception found in:\" + folder_run, \"at index:\" + str(trial_index))\r\n",
        "      print(\"Building a replacement line from index: \" + str(trial_index-1))\r\n",
        "      # if the row fails, just copy the previous row values to fill broken values\r\n",
        "      for index in subset_indexes:\r\n",
        "        csv_file.write(str(data[trial_index-1][index]))\r\n",
        "        if index != subset_indexes.size-1:\r\n",
        "          csv_file.write(',')\r\n",
        "      csv_file.write('\\n')\r\n",
        "      break"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPHs_rw9UUq1"
      },
      "source": [
        "# Run this cell to navigate through the Drive filesystem and extract a subset of the dataset for a specific task\r\n",
        "\r\n",
        "dataset_path = '/content/drive/MyDrive/ml2-eeg-biometrics/raw-data/'\r\n",
        "destination_path = '/content/drive/MyDrive/'\r\n",
        "file_name = 'eeg_dataset_right_hand_task_subset_9channels.csv'\r\n",
        "\r\n",
        "task_duration = 5 # Task average duration is 5  seconds\r\n",
        "\r\n",
        "dataset = {}\r\n",
        "\r\n",
        "left_hand_task = 769\r\n",
        "right_hand_task = 770\r\n",
        "both_feet_task = 771\r\n",
        "idle_task = 780\r\n",
        "\r\n",
        "labels = np.array(['Participant', 'Date', 'Run', 'Task','Trial','F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4'])\r\n",
        "subset_indexes = np.array([0, 4, 6, 10, 13, 15, 17, 20, 24])\r\n",
        "folder_list = os.listdir(dataset_path)\r\n",
        "\r\n",
        "generate_csv(destination_path, file_name, labels, subset_indexes, folder_list, task_duration, right_hand_task)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PECkF9bNUkqD"
      },
      "source": [
        "# Run this cell to navigate through the Drive filesystem and extract the dataset for a specific task\r\n",
        "\r\n",
        "dataset_path = '/content/drive/MyDrive/ml2-eeg-biometrics/raw-data/'\r\n",
        "\r\n",
        "task_duration = 5 # Task average duration is 5  seconds\r\n",
        "\r\n",
        "dataset = {}\r\n",
        "\r\n",
        "left_hand_task = 769\r\n",
        "right_hand_task = 770\r\n",
        "both_feet_task = 771\r\n",
        "idle_task = 780\r\n",
        "\r\n",
        "labels = np.array(['Participant', 'Date', 'Run', 'Task','Trial','F3', 'F1', 'Fz', 'F2', 'F4', 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'hEOG', 'vEOG', 'F5', 'AF3', 'AF4', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'PO3', 'POz', 'PO4', 'Oz', 'F6'])\r\n",
        "folder_list = os.listdir(dataset_path)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpVYmRl0cQNN"
      },
      "source": [
        "# Dataset reader\r\n",
        "for participant in dataset:\r\n",
        "  print(participant)\r\n",
        "  for date in dataset[participant]:\r\n",
        "    print(date)\r\n",
        "    for trial in dataset[participant][date]:\r\n",
        "      print(trial + \":\")\r\n",
        "      print(dataset[participant][date][trial])\r\n",
        "    print(\"\\n\")\r\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6khd7zwAW_H8"
      },
      "source": [
        "example_file_name = '/content/drive/MyDrive/ml2-eeg-biometrics/raw-data/S1/S1_20200724/NSsignal_2020_07_24_10_19_46.npz'\r\n",
        "\r\n",
        "npz_data = np.load(example_file_name)\r\n",
        "npz_data_dict = dict(npz_data)\r\n",
        "sample_rate = npz_data_dict['SampleRate'][0]\r\n",
        "marks = npz_data_dict['MarkOnSignal'] \r\n",
        "data = npz_data_dict['signal']\r\n",
        "print(npz_data_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw7uCGxjFH7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3789db7-288a-42fb-b36c-a35741ed4411"
      },
      "source": [
        "# Run this cell to save the changes\r\n",
        "\r\n",
        "drive.flush_and_unmount()\r\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}