{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/eeg_biometrics_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTCrOSwlbGd"
      },
      "source": [
        "## Load libraries & initialise environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQFvwJpfLSt"
      },
      "source": [
        "# import libraries\r\n",
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "sns.set(color_codes=True)\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, QuantileTransformer\r\n",
        "from copy import deepcopy\r\n",
        "from sklearn.externals import joblib\r\n",
        "from numpy.random import seed\r\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, Reshape, \\\r\n",
        "          RepeatVector, MaxPooling1D, Conv1D, Flatten, Conv1DTranspose, UpSampling1D, \\\r\n",
        "          AveragePooling1D\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras import regularizers\r\n",
        "from keras import backend as K\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "# print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXQT6VF9fRAK"
      },
      "source": [
        "drive.mount(\"/content/drive\")\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6YB2XBaXlTy"
      },
      "source": [
        "# Seed value\r\n",
        "# Apparently you may use different seed values at each stage\r\n",
        "seed_value = 10\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
        "\r\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "\r\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\r\n",
        "\r\n",
        "random.seed(seed_value)\r\n",
        "\r\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\r\n",
        "np.random.seed(seed_value)\r\n",
        "\r\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\r\n",
        "tf.compat.v1.set_random_seed(seed_value)\r\n",
        "\r\n",
        "# 5. Configure a new global `tensorflow` session\r\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\r\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeTPxR0aYNxV"
      },
      "source": [
        "## Load & Process Data\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BInAGYqORGnb"
      },
      "source": [
        "##### Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlN5H_lIhPin"
      },
      "source": [
        "To begin with, we load the 3 parts of the dataset, training, test and validation that we split in the pre-processing phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqlAto997Wp"
      },
      "source": [
        "def load_datasets():\r\n",
        "  x_train = np.asarray(np.load(dirpath + 'x_train.npy')).astype(np.float32)\r\n",
        "  x_test = np.asarray(np.load(dirpath + 'x_test.npy', allow_pickle=True)).astype(np.float32)\r\n",
        "  x_valid = np.asarray(np.load(dirpath + 'x_valid.npy', allow_pickle=True)).astype(np.float32)\r\n",
        "  return x_train, x_test, x_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn-ErKYzhZZx"
      },
      "source": [
        "x_train_unscaled, x_test_unscaled, x_valid_unscaled  = load_datasets()\r\n",
        "print(\"Data loaded. Shapes:\")\r\n",
        "print(x_train_unscaled.shape, x_test_unscaled.shape, x_valid_unscaled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EporEztRJ5L"
      },
      "source": [
        "##### Plot distributions of unscaled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je6ycNzG3EMw"
      },
      "source": [
        "cols = ['Statistic','F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "def data_summary(dataset, label):\r\n",
        "  \"\"\" \r\n",
        "  input:\r\n",
        "    dataset     the three dimensional input (n_samples, n_timepoints, n_features) \r\n",
        "\r\n",
        "    Prints histograms for the 9 features individually\r\n",
        "  returns: \r\n",
        "    summ_df     pd.DataFrame containing summary statistics for the 9 features.\r\n",
        "  \"\"\"\r\n",
        "  data = dataset.reshape((dataset.shape[0] * dataset.shape[1], dataset.shape[2])) # Reshape to 2D (n_samples*n_timepoints, n_features)\r\n",
        "  \r\n",
        "  # Calculate the summary statistics.\r\n",
        "  min   = data.min(axis=0).reshape(1, data.shape[1])                  # Calculate the minimum over the rows for each column.\r\n",
        "  max   = data.max(axis=0).reshape(1, data.shape[1])                  # Then reshape the result to one row and n_cols=n_features, to make it easier to combine later.\r\n",
        "  mean  = data.mean(axis=0).reshape(1, data.shape[1])\r\n",
        "  var   = data.var(axis=0).reshape(1, data.shape[1])\r\n",
        "  q01   = np.quantile(data, 0.01, axis=0).reshape(1, data.shape[1])\r\n",
        "  q99   = np.quantile(data, 0.99, axis=0).reshape(1, data.shape[1])\r\n",
        "\r\n",
        "  names=np.array([['min','max','mean','var','1st percentile', '99th percentile']]).reshape(6,1) # Create a column of names for the summary stats.\r\n",
        "  stats = np.concatenate((min,max,mean,var,q01,q99), axis=0)          # Combine the summary stats in one array\r\n",
        "\r\n",
        "  summ = np.concatenate((names, np.round(stats, 4)), axis=1)          # Combine the summary stats with their names.\r\n",
        "  summ_df = pd.DataFrame(summ, columns=cols)                          # Create a dataframe and supply the channel names as columns.\r\n",
        "\r\n",
        "  # Plot histograms per channel.\r\n",
        "  fig, axes = plt.subplots(3,3, figsize = (9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "  for i in range(9): # Loop through the channels.\r\n",
        "    axes[i].hist(data[:,i], range= (q01[0,i], q99[0,i]),   density=True)    # Add histogram subplot for the values of that channel.\r\n",
        "    axes[i].title.set_text(cols[i+1])                                       # Add a title with the channel name.\r\n",
        "  fig.suptitle(\"Distribution for each channel (between 1st & 99th percentile) of \" + label + \" dataset\" , size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])                                 # Cut the plot space to make space for the global title.\r\n",
        "\r\n",
        "  return summ_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPhAKBtY04dN"
      },
      "source": [
        "# Plot distributions of each channel.\r\n",
        "unscaled_training_summary = data_summary(x_train_unscaled, \"Training\")\r\n",
        "unscaled_test_summary = data_summary(x_test_unscaled, \"Test\")\r\n",
        "unscaled_valid_summary = data_summary(x_valid_unscaled, \"Validation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obsst4fTzSK4"
      },
      "source": [
        "### Band-pass filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6M8UO7FRueQ"
      },
      "source": [
        "##### Create the filters and apply across the whole data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMKQR0SzQ47"
      },
      "source": [
        "from scipy import signal\r\n",
        "from copy import deepcopy\r\n",
        "\r\n",
        "low_cut = 0.1\r\n",
        "high_cut = 50.0\r\n",
        "\r\n",
        "bp = signal.butter(10, (low_cut,high_cut), 'bp', fs=500, output='sos') # Create the filter. fs is the sampling rate.\r\n",
        "\r\n",
        "# Create copies of the data\r\n",
        "x_train_filtered = deepcopy(x_train_unscaled)         # After running once in the session, I comment these out because otherwise if you re-run the cell it eats RAM.\r\n",
        "x_test_filtered = deepcopy(x_test_unscaled)\r\n",
        "x_valid_filtered = deepcopy(x_valid_unscaled)\r\n",
        "\r\n",
        "print(x_train_filtered.shape)\r\n",
        "\r\n",
        "x_train_filtered = signal.sosfilt(bp, x_train_filtered, axis=1)\r\n",
        "x_valid_filtered = signal.sosfilt(bp, x_valid_filtered, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jttdl9hv6zj"
      },
      "source": [
        "# How many values lay beyond a threshold?\r\n",
        "threshold = 85\r\n",
        "\r\n",
        "outliers_pre_BP = abs(x_train_unscaled) > threshold\r\n",
        "outliers_post_BP = abs(x_train_filtered) > threshold\r\n",
        "                                \r\n",
        "print(\"Before BP filtering, {0:.2f}% of all values lay outside ±{1}\".format((100*np.sum(outliers_pre_BP))/(6665*2500*9), threshold))\r\n",
        "print(\"After BP filtering, {0:.2f}% of all values lay outside ±{1}\".format((100*np.sum(outliers_post_BP))/(6665*2500*9), threshold))\r\n",
        "\r\n",
        " # Number of samples with at least value outside threshold.\r\n",
        "print(\"After BP filtering, there are {0} samples with at least one value outside ±{1}\".format(np.sum(np.max(outliers_post_BP, axis=(1,2))), threshold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQIwyM7tCAp"
      },
      "source": [
        "#### Apply scaling with quantile transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AShIoKEB5_lh"
      },
      "source": [
        "transformer = QuantileTransformer(output_distribution='normal')\r\n",
        "\r\n",
        "# Create copies so the original data remains unaltered.\r\n",
        "x_train = deepcopy(x_train_filtered)\r\n",
        "x_valid = deepcopy(x_valid_filtered)\r\n",
        "x_test = deepcopy(x_test_filtered)\r\n",
        "\r\n",
        "# Get the dimensionality for re-shaping.\r\n",
        "n_samples, n_timepoints, n_features = x_train.shape\r\n",
        "n_samples_valid = x_valid.shape[0]\r\n",
        "n_samples_test = x_test.shape[0]\r\n",
        "\r\n",
        "# Re-shape to 2D for the scaler.\r\n",
        "x_train = x_train.reshape((n_samples*n_timepoints, n_features))        \r\n",
        "x_valid = x_valid.reshape((n_samples_valid*n_timepoints, n_features))\r\n",
        "x_test = x_test.reshape((n_samples_test*n_timepoints, n_features))\r\n",
        "\r\n",
        "# Fit and apply the scaler/transformer to the datasets.\r\n",
        "x_train = transformer.fit_transform(x_train)         \r\n",
        "x_valid = transformer.transform(x_valid)            \r\n",
        "x_test = transformer.transform(x_test)    \r\n",
        "\r\n",
        "# Re-shape to 3D for input to the convolutional autoencoder.\r\n",
        "x_train = x_train.reshape((n_samples, n_timepoints, n_features)) \r\n",
        "x_valid = x_valid.reshape((n_samples_valid, n_timepoints, n_features)) \r\n",
        "x_test = x_test.reshape((n_samples_test, n_timepoints, n_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZQI-k4G6WYu"
      },
      "source": [
        "# data_summary(x_train, \"Training Filtered\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOK0DnGQR95A"
      },
      "source": [
        "##### Plot distributions of the total dataset after the band-pass filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_WtSqx15NJ"
      },
      "source": [
        "# x_train_filtered_summary = data_summary(x_train_filtered, \"Training Filtered\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IW7QFavd7M"
      },
      "source": [
        "### Smooth out extreme points beyond the 1st-99th percentile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbOssMQHviNv"
      },
      "source": [
        "# # Reshape to 2 dimensional array\r\n",
        "# x_train = deepcopy(x_train_unscaled)\r\n",
        "# x_test = deepcopy(x_test_unscaled)\r\n",
        "# x_valid = deepcopy(x_valid_unscaled)\r\n",
        "\r\n",
        "# n_samples, n_timepoints, n_features = x_train.shape\r\n",
        "# n_samples_test =  x_test.shape[0]\r\n",
        "# n_samples_valid = x_valid.shape[0]\r\n",
        "\r\n",
        "# x_train = x_train.reshape((n_samples*n_timepoints, n_features))\r\n",
        "# x_test = x_test.reshape((n_samples_test*n_timepoints, n_features))\r\n",
        "# x_valid = x_valid.reshape((n_samples_valid*n_timepoints, n_features))\r\n",
        "\r\n",
        "# # Find the 1st & 99th percentiles for each column of the training data.\r\n",
        "# q01  = np.quantile(x_train, 0.01, axis=0)\r\n",
        "# q99  = np.quantile(x_train, 0.99, axis=0)\r\n",
        "\r\n",
        "# # Loop through the columns and apply the cutoff\r\n",
        "# for i in range(x_train.shape[1]):\r\n",
        "#   x_train[x_train[:,i] < q01[i], i] = q01[i] # If the value is below the 1st percentile, replace with the 1st percentile.\r\n",
        "#   x_train[x_train[:,i] > q99[i], i] = q99[i] # If the value is above the 99th percentile, replace with the 99th percentile.\r\n",
        "#   # Do the same with the test and validation data, using the cutoffs calculated from the training data.\r\n",
        "#   x_test[x_test[:,i] < q01[i], i] = q01[i] \r\n",
        "#   x_test[x_test[:,i] > q99[i], i] = q99[i] \r\n",
        "#   x_valid[x_valid[:,i] < q01[i], i] = q01[i] \r\n",
        "#   x_valid[x_valid[:,i] > q99[i], i] = q99[i] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpn8rNubT0LF"
      },
      "source": [
        "### Scale the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6O7PMgjVYEy"
      },
      "source": [
        "##### Scale the artificially smoothed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzi-3d-CxyKt"
      },
      "source": [
        "# # normalize the data\r\n",
        "# scaler = MinMaxScaler()\r\n",
        "\r\n",
        "# x_train = scaler.fit_transform(x_train)\r\n",
        "# x_test = scaler.transform(x_test)\r\n",
        "# x_valid = scaler.transform(x_valid)\r\n",
        "# scaler_filename = \"/content/drive/MyDrive/ml2-eeg-biometrics/scaler_data\"\r\n",
        "# joblib.dump(scaler, scaler_filename)\r\n",
        "\r\n",
        "# # Reshape to 3D for the convolutional autoencoder\r\n",
        "# x_train = x_train.reshape((n_samples, n_timepoints, n_features))\r\n",
        "# x_test = x_test.reshape((n_samples_test, n_timepoints, n_features))\r\n",
        "# x_valid = x_valid.reshape((n_samples_valid, n_timepoints, n_features))\r\n",
        "# print(\"x_train shape: \", x_train.shape, \"x_test shape:\", x_test.shape, \"x_valid shape:\", x_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGo8tjJwVVjZ"
      },
      "source": [
        "##### Scale the filtered data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fvTtfV8evvj"
      },
      "source": [
        "# # Scaling applied to the filtered signal.\r\n",
        "# scaler_filtered = MinMaxScaler()\r\n",
        "\r\n",
        "# n_samples, n_timepoints, n_features = x_train_filtered.shape\r\n",
        "# n_samples_test =  x_test_filtered.shape[0]\r\n",
        "\r\n",
        "# # Reshape to 2D for the scaler.\r\n",
        "# x_train_filtered = x_train_filtered.reshape((n_samples*n_timepoints, n_features))\r\n",
        "# # x_test_filtered = x_test_filtered.reshape((n_samples_test*n_timepoints, n_features))\r\n",
        "\r\n",
        "# # Apply the scaling.\r\n",
        "# x_train_filtered = scaler_filtered.fit_transform(x_train_filtered)\r\n",
        "# # x_test_filtered = scaler_filtered.transform(x_test_filtered)\r\n",
        "\r\n",
        "# # Re-shape back to 3D for the convolutional autoencoder.\r\n",
        "# x_train_filtered = x_train_filtered.reshape((n_samples, n_timepoints, n_features))    \r\n",
        "# # x_test_filtered = x_test_filtered.reshape((n_samples_test, n_timepoints, n_features))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Xfm82dUNWF"
      },
      "source": [
        "# data_summary(x_train)     # Plot the distribution of the scaled data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7XEKeEo8a5"
      },
      "source": [
        "##### Visualise the Raw & Scaled Signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DEghaTsYrFO"
      },
      "source": [
        "labels=[(('F3',0), ('F4',1)), (('FC3',2), ('FC4',3)), (('C3',4), ('Cz',5), ('C4',6)), (('CP3',7), ('CP4',8))]\r\n",
        "colours= ['darkslateblue', 'orange','lightskyblue','brown','darkgreen','darkgrey','bisque','violet','palegreen']\r\n",
        "\r\n",
        "def plot_signals(sample, title=None):\r\n",
        "  fig, axes = plt.subplots(2,2, figsize = (6,6))\r\n",
        "  axes=axes.ravel()\r\n",
        "  plt.suptitle(\"Signals\" if title is None else title, size=16)\r\n",
        "  count=0\r\n",
        "  for label_group in labels:\r\n",
        "    for label, ind in label_group:\r\n",
        "      axes[count].plot(sample[:,ind], label=label,color=colours[ind], alpha=0.8)\r\n",
        "      axes[count].legend()\r\n",
        "    count+=1\r\n",
        "\r\n",
        "# plot_signals(x_train[101], title=\"Scaled Signals - x_train[0]\")\r\n",
        "plot_signals(x_train_unscaled[101], title=\"Unscaled Signals - x_train[0]\")\r\n",
        "plot_signals(x_train_filtered[101], title=\"Filtered Signals - x_train[0]\")\r\n",
        "plot_signals(x_train[101], title=\"Scaled Signals - x_train[0]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PWDBiuWVX8"
      },
      "source": [
        "## Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUsOKCEyxv1"
      },
      "source": [
        "#### Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltaV2CiYXS5J"
      },
      "source": [
        "##### Define a convolutional autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xX-IPc1GXH"
      },
      "source": [
        "This object-orientd implementation of the Convolutional Autoencoder allows to build an extra candidate architecture to be compared with the current architecture. In additions it includes several methods (that wrap native methdos from the class Model) to plot, evaluate and save a model easily while performing an experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjpduhZRc2TF"
      },
      "source": [
        "\r\n",
        "class ConvAutoencoder():\r\n",
        "\r\n",
        "  def __init__(self, loss, optimizer, shape, candidate=False):\r\n",
        "      self.input_shape = shape   \r\n",
        "      self.encoder = None\r\n",
        "      self.history = None\r\n",
        "      print(\"Candidate build:\", candidate)    \r\n",
        "      self.autoencoder_model = self.build_candidate_model() if candidate else self.build_model()\r\n",
        "      self.autoencoder_model.compile(loss=loss, optimizer=optimizer)\r\n",
        "      self.autoencoder_model.summary()\r\n",
        "  \r\n",
        "\r\n",
        "  ''' Builds the architecture of the model'''   \r\n",
        "  def build_model(self):\r\n",
        "      input_layer = Input(shape=self.input_shape)\r\n",
        "      # encoder\r\n",
        "      e1 = Conv1D(filters=18, kernel_size=3, activation='relu', padding='same',dilation_rate=2)(input_layer) # When using this layer as the first layer in a model, provide an input_shape argument \r\n",
        "      e2 = MaxPooling1D(pool_size=2)(e1) # Downsamples the input representation by taking the maximum value over the window defined by pool_size. The window is shifted by strides. \r\n",
        "      e3 = Conv1D(filters=9,kernel_size=3, activation='relu', padding='same',dilation_rate=2)(e2)\r\n",
        "      e4 = MaxPooling1D(pool_size=2)(e3)\r\n",
        "      e5 = Conv1D(filters=5,kernel_size=3, activation='relu', padding='same',dilation_rate=2)(e4)\r\n",
        "      e6 = MaxPooling1D(pool_size=5)(e5)\r\n",
        "      e7 = Conv1D(filters=2,kernel_size=3, activation='relu', padding='same',dilation_rate=2)(e6)\r\n",
        "      #e7 = AveragePooling1D()(e6)\r\n",
        "      encoded = Flatten()(e7)\r\n",
        "      self.encoder = encoded\r\n",
        "      #encoded = Dense(units=80)(flat)\r\n",
        "\r\n",
        "      # decoder\r\n",
        "      #d1 = Dense(units=150)(encoded) # Densely-connected NN layer.\r\n",
        "      d1 = Reshape((125,2))(encoded) #Layer that reshapes inputs into the given shape.\r\n",
        "      d2 = UpSampling1D(size=2)(d1)\r\n",
        "      d3 = Conv1D(filters=5,kernel_size=3,strides=1, activation='relu', padding='same')(d2)\r\n",
        "      d4 = UpSampling1D(size=5)(d3) #Repeats each temporal step size times along the time axis.\r\n",
        "      d5 = Conv1D(filters=9,kernel_size=3,strides=1, activation='relu', padding='same')(d4)\r\n",
        "      d6 = UpSampling1D(size=2)(d5)\r\n",
        "      decoded = Conv1D(filters=9,kernel_size=3,strides=1, activation='sigmoid', padding='same')(d6)\r\n",
        "\r\n",
        "      model = Model(inputs=input_layer, outputs=decoded)\r\n",
        "      model.output_shape\r\n",
        "      return model\r\n",
        "\r\n",
        "\r\n",
        "  ''' Builds the architecture of a candidate model, to be used in comparison with the current model'''   \r\n",
        "  def build_candidate_model(self):\r\n",
        "      input_layer = Input(shape=self.input_shape)\r\n",
        "      e1 = Conv1D(18, 9, activation='relu', padding='same')(input_layer)\r\n",
        "      e2 = MaxPooling1D(2)(e1)\r\n",
        "      e3 = Conv1D(6,5, activation = 'relu', padding='same')(e2)\r\n",
        "      e4 = MaxPooling1D(5)(e3)\r\n",
        "      e5 = Conv1D(2,3, activation = 'relu', padding='same')(e4)\r\n",
        "      e6 = MaxPooling1D(2)(e5)\r\n",
        "      encoded = Flatten()(e6)\r\n",
        "      # encoded = Dense(100, activation='relu')(e5)\r\n",
        "      # d1 = Dense(100, activation='relu')(encoded)\r\n",
        "      d0 = Reshape((125,2))(encoded)\r\n",
        "      d1 = UpSampling1D(2)(d0)\r\n",
        "      d2 = Conv1DTranspose(6, 5, activation='relu', padding='same')(d1)    \r\n",
        "      d3 = UpSampling1D(5)(d2)\r\n",
        "      d4 = Conv1DTranspose(18, 5, activation='relu', padding='same')(d3)\r\n",
        "      d5 = UpSampling1D(2)(d4)\r\n",
        "      d6 = Conv1DTranspose(9, 9, activation='linear', padding='same')(d5)\r\n",
        "      # decoded = Dense(500, activation='sigmoid')(d6)\r\n",
        "\r\n",
        "      model = Model(inputs=input_layer, outputs=d6)\r\n",
        "      model.output_shape\r\n",
        "      return model\r\n",
        "\r\n",
        "\r\n",
        "  ''' Trains the model on the full dataset, requires validation split'''                     \r\n",
        "  def train_model(self, x_train, x_val, epochs, batch_size=20):\r\n",
        "      early_stopping = EarlyStopping(monitor='loss',\r\n",
        "                                      min_delta=0,\r\n",
        "                                      patience=5,\r\n",
        "                                      verbose=1, \r\n",
        "                                      mode='auto')\r\n",
        "      self.history = self.autoencoder_model.fit(x_train, x_train,\r\n",
        "                                            batch_size=batch_size,\r\n",
        "                                            epochs=epochs,\r\n",
        "                                            validation_data=(x_val, x_val),\r\n",
        "                                            callbacks=[early_stopping])\r\n",
        "      return self.history\r\n",
        "\r\n",
        "\r\n",
        "  ''' Trains the model on a single channel, no validation split'''   \r\n",
        "  def train_subset(self, x_train, epochs, batch_size=20):\r\n",
        "      history = self.autoencoder_model.fit(x_train[:,:,0], x_train[:,:,0], epochs=epochs,\r\n",
        "                  batch_size=batch_size,\r\n",
        "                  # validation_split=0.05\r\n",
        "                  )\r\n",
        "      \r\n",
        "\r\n",
        "  ''' Plots the history.\r\n",
        "  input:\r\n",
        "    history: if present, plot the history given as parameter\r\n",
        "  output:\r\n",
        "    None\r\n",
        "  '''       \r\n",
        "  def plot_history(self, history=None):\r\n",
        "      history = history if history is not None else self.history\r\n",
        "      plt.plot(history.history['loss'])\r\n",
        "      plt.plot(history.history['val_loss'])\r\n",
        "      plt.title('Model loss')\r\n",
        "      plt.ylabel('Loss')\r\n",
        "      plt.xlabel('Epoch')\r\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "      plt.show()\r\n",
        "      \r\n",
        "\r\n",
        "  ''' Return the predictions for the trained data.\r\n",
        "   input:\r\n",
        "    X: the training data\r\n",
        "   output:\r\n",
        "    preds: predictions for the given training data\r\n",
        "  '''       \r\n",
        "  def eval_model(self, X):\r\n",
        "      preds = self.autoencoder_model.predict(X)\r\n",
        "      return preds\r\n",
        "\r\n",
        "\r\n",
        "  ''' Save all model information, including weights, in h5 format\r\n",
        "    input:\r\n",
        "      path: full path on drive, for example /content/drive/MyDrive/ml2-eeg-biometrics/saved_models/\r\n",
        "    output:\r\n",
        "      None\r\n",
        "  '''    \r\n",
        "  def save_model(self, path):\r\n",
        "      timestamp = pd.Timestamp.now()\r\n",
        "      model_name = self.autoencoder_model.name + \"_\" + str(timestamp) + \".h5\"\r\n",
        "      saved_model = self.autoencoder_model.save(path + model_name)\r\n",
        "      print(\"Model saved at \" + str(timestamp))\r\n",
        "\r\n",
        "\r\n",
        "  ''' Retrieve the history of the model.\r\n",
        "  input:\r\n",
        "    None\r\n",
        "  output:\r\n",
        "    history: history saved in the object model\r\n",
        "  '''\r\n",
        "  def retrieve_history(self):\r\n",
        "      return self.history\r\n",
        "\r\n",
        "\r\n",
        "  ''' Retrieve the encoder part of the model.\r\n",
        "  input:\r\n",
        "    index: the index of the encoded layer\r\n",
        "  output:\r\n",
        "    encoder: a model built with the input layer as input and the encoded layer as output\r\n",
        "  '''\r\n",
        "  def retrieve_encoder(self, index):\r\n",
        "      encoder = Model(self.autoencoder_model.input, self.autoencoder_model.get_layer(index=index).output)\r\n",
        "      return encoder\r\n",
        "  \r\n",
        "\r\n",
        "  ''' Assess the model on a subset of data\r\n",
        "  input:\r\n",
        "    X: the training data\r\n",
        "  output:\r\n",
        "    None\r\n",
        "  '''\r\n",
        "  def assess_subset(self, X):\r\n",
        "    x_pred = self.eval_model(X)\r\n",
        "    for i in range(0,len(x_1),200):\r\n",
        "      plt.plot(x_1[i,:,0], label='actual')\r\n",
        "      plt.plot(x_pred[i,:,0], label = 'predicted')\r\n",
        "      plt.show()\r\n",
        "\r\n",
        "\r\n",
        "  ''' Plot prediction for the model.\r\n",
        "  input:\r\n",
        "    X: the training data\r\n",
        "  output:\r\n",
        "    None\r\n",
        "  '''\r\n",
        "  def plot_prediction(self, X):\r\n",
        "    x_pred = candidate_model.eval_model(X)\r\n",
        "    plt.figure(figsize=(14,8))\r\n",
        "    plt.plot(X[1039,:,0], label='actual', alpha=0.7)\r\n",
        "    plt.plot(x_pred[1039,:,0], label= 'predicted',alpha=0.7)\r\n",
        "    # plt.ylim(0.475, 0.495)\r\n",
        "    plt.legend()\r\n",
        "\r\n",
        "  ''' Plot the distribution of the loss across the dataset\r\n",
        "  input:\r\n",
        "    X_pred: prediction on the training data (obtainable from eval_model())\r\n",
        "    X_train: the training data\r\n",
        "  output:\r\n",
        "    None\r\n",
        "  '''\r\n",
        "  def plot_loss_distribution(self, X_pred, x_train):\r\n",
        "    x_train_reshaped = x_train.reshape(x_train.shape[0]*x_train.shape[1], x_train.shape[2])\r\n",
        "    loss_mae = np.mean(np.abs(X_pred-x_train_reshaped), axis = 1)\r\n",
        "    plt.figure(figsize=(16,9), dpi=80)\r\n",
        "    plt.title('Loss Distribution', fontsize=16)\r\n",
        "    sns.distplot(loss_mae, bins = 20, kde= True, color = 'blue');\r\n",
        "    plt.xlim([0.0,.5])\r\n",
        "    \r\n",
        "\r\n",
        "  \"\"\" Function to plot predictions vs. the actuals for one sample.\r\n",
        "  input:\r\n",
        "    actuals   3D array (n_samplesx2500x9) - Original scaled signals.\r\n",
        "    pred      the predicted values corresponding to the actuals.\r\n",
        "    ind     The row number of the sample (2500x9) that you want to compare.\r\n",
        "    rescale   If set to true then the data is first converted back to the original scale for comparison.\r\n",
        "  \r\n",
        "  returns:\r\n",
        "    nothing\r\n",
        "\r\n",
        "  prints plots.\r\n",
        "  \"\"\"\r\n",
        "  def evaluate_prediction(self, actuals, pred, ind, rescale=False):\r\n",
        "    cols = ['F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "\r\n",
        "    if rescale:\r\n",
        "      sample_actual = scaler.inverse_transform(actuals[ind]) # Rescale to the original scale.\r\n",
        "      sample_pred = scaler.inverse_transform(pred[ind]) \r\n",
        "    else:\r\n",
        "      sample_actual = actuals[ind]    # Get the relevant sample.\r\n",
        "      sample_pred = pred[ind]\r\n",
        "\r\n",
        "    mae_by_channel = np.mean(np.abs(sample_pred - sample_actual), axis=0) # Get the Mean Absolute Error for each channel for this sample\r\n",
        "    sample_mae = np.mean(mae_by_channel) # Get the total MAE for the sample by taking the average across the 9 channels\r\n",
        "    print(\"Sample\", ind, \"\\n   Total Mean Absolute Error:\", round(sample_mae, 8))\r\n",
        "    print(\"Mean Absolute Error by Channel:\")\r\n",
        "    for col, error in zip(cols, mae_by_channel):\r\n",
        "      print(col, \": \", round(error,8)) \r\n",
        "\r\n",
        "    fig, axes = plt.subplots(3,3, figsize=(9,9))\r\n",
        "    axes=axes.ravel()\r\n",
        "\r\n",
        "    for i in range(9):\r\n",
        "      axes[i].plot(sample_actual[:,i], label= \"Actual\")\r\n",
        "      axes[i].plot(sample_pred[:,i], label=\"Predicted\")\r\n",
        "      axes[i].title.set_text(cols[i] + str(round(mae_by_channel[i], 3)))\r\n",
        "    \r\n",
        "    plt.legend()\r\n",
        "    fig.suptitle(\"Predictions vs. Actuals - Sample \" + str(ind),size=16)\r\n",
        "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\r\n",
        "\r\n",
        "  ''' Plot loss distribution of the test set for each channel.\r\n",
        "  input:\r\n",
        "    X_test: the test data \r\n",
        "  output:\r\n",
        "    None\r\n",
        "  '''\r\n",
        "  def plot_loss_test(self, x_test):\r\n",
        "    X_pred = self.eval_model(x_test)\r\n",
        "    X_pred = X_pred.reshape(X_pred.shape[0]*X_pred.shape[1], X_pred.shape[2])\r\n",
        "\r\n",
        "    x_test_reshaped = x_test.reshape(x_test.shape[0]*x_test.shape[1], x_test.shape[2])\r\n",
        "    fig, axes = plt.subplots(9,1, figsize=(18,9))\r\n",
        "    # Plot the loss distribution for each channel individually\r\n",
        "    for i in range(x_test_reshaped.shape[1]):\r\n",
        "      loss_mae = np.abs(X_pred[:,i]-x_test_reshaped[:,i])\r\n",
        "      sns.distplot(loss_mae, bins = 100, kde= True, color = 'blue', ax=axes[i]);\r\n",
        "      axes[i].axis(xmin=0.0,xmax=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUwCCZ0ZXxdR"
      },
      "source": [
        "##### Fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbFLfZEL_Gdh"
      },
      "source": [
        "# create the autoencoder model with the current architecture\r\n",
        "optimizer = Adam(lr=0.001)\r\n",
        "shape = (x_train.shape[1], x_train.shape[2])\r\n",
        "model = ConvAutoencoder(loss='mae', optimizer=optimizer, shape=shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzzXJTvNyAn0"
      },
      "source": [
        "# create the autoencoder model with the candidate architecture\r\n",
        "optimizer = Adam(lr=0.001)\r\n",
        "shape = (x_train.shape[1], x_train.shape[2])\r\n",
        "candidate_model = ConvAutoencoder(loss='mae', optimizer=optimizer, shape=shape, candidate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDgOztHOHazh"
      },
      "source": [
        "# Train both current and candidate models\r\n",
        "\r\n",
        "# fit the model to the data\r\n",
        "nb_epochs = 10\r\n",
        "batch_size = 20\r\n",
        "x_1 = x_train[:,:,:]\r\n",
        "## Train the models on the full dataset\r\n",
        "print('Training Current model for ' + str(nb_epochs) + \" epochs and batch size of \" + str(batch_size))\r\n",
        "# model.train_model(x_1, x_valid, nb_epochs, batch_size)\r\n",
        "print('============================================================')\r\n",
        "print('Training Candidate model for ' + str(nb_epochs) + \" epochs and batch size of \" + str(batch_size))\r\n",
        "candidate_model.train_model(x_1, x_valid, nb_epochs, batch_size)\r\n",
        "print('============================================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64K4Tw5CcXZs"
      },
      "source": [
        "# Obtain the encoder part of the models.\r\n",
        "encoder = candidate_model.retrieve_encoder(index=7)\r\n",
        "# encoder.summary()\r\n",
        "\r\n",
        "train_pred = encoder.predict(x_train)\r\n",
        "valid_pred = encoder.predict(x_valid)\r\n",
        "\r\n",
        "timestamp = pd.Timestamp.now()\r\n",
        "np.save(dirpath + \"train_encoding_\" + candidate_model.autoencoder_model.name + \"_\" + str(timestamp) + \".npy\", train_pred)\r\n",
        "np.save(dirpath + \"valid_encoding_\" + candidate_model.autoencoder_model.name + \"_\" + str(timestamp) + \".npy\", valid_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKottLNgXpw6"
      },
      "source": [
        "model.plot_history()\r\n",
        "candidate_model.plot_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGDjFwgX8pZ"
      },
      "source": [
        "##### Plot the reconstruction for individual samples.\r\n",
        "Temporary code for assessing the models on the subset of data (one channel and only 20% of timepoints)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hm2DWhiSmeV"
      },
      "source": [
        "# model.assess_subset(x_1)\r\n",
        "candidate_model.assess_subset(x_1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yefRYH1sc5LY"
      },
      "source": [
        "Plot prediction for the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbHkzSliPCa"
      },
      "source": [
        "model.plot_prediction(x_1)\r\n",
        "candidate_model.plot_prediction(x_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfIG-Na0Rim"
      },
      "source": [
        "\r\n",
        "## Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AvXQtdVLPT"
      },
      "source": [
        "# Get the predicted values for the training set.\r\n",
        "X_pred_3D = model.eval_model(x_train)\r\n",
        "# X_pred_3D = candidate_model.eval_model(x_train_f)\r\n",
        "X_pred = X_pred_3D.reshape(X_pred_3D.shape[0]*X_pred_3D.shape[1], X_pred_3D.shape[2])\r\n",
        "# X_pred = pd.DataFrame(X_pred, columns=train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIl53WipVnBA"
      },
      "source": [
        "##### Plot distribution of the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERj4N100Maz"
      },
      "source": [
        "# Plot the distribution of the loss\r\n",
        "model.plot_loss_distribution(X_pred, x_train)\r\n",
        "# candidate_model.plot_loss_distribution(X_pred, x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uQFT7_1Vtt1"
      },
      "source": [
        "##### Evaluate total re-construction for one sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYRpNQjWF-u"
      },
      "source": [
        "model.evaluate_prediction(x_train, X_pred_3D, ind=1, rescale=False)\r\n",
        "# candidate_model.evaluate_prediction(x_train, X_pred_3D, ind=1, rescale=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ehGjGxUV4Vf"
      },
      "source": [
        "##### Plot the loss distribution of the test set (code from tutorial, needs to be edited)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2FB1JADjIE"
      },
      "source": [
        "# plot the loss distribution of the test set\r\n",
        "model.plot_loss_test(x_test)\r\n",
        "# candidate_model.plot_loss_test(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVIfqljx0lwt"
      },
      "source": [
        "model.save_model(\"/content/drive/MyDrive/ml2-eeg-biometrics/saved_models/\") # Get current timestamp and model number.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}