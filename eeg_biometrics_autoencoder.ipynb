{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/eeg_biometrics_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTCrOSwlbGd"
      },
      "source": [
        "## Load libraries & initialise environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQFvwJpfLSt"
      },
      "source": [
        "# import libraries\r\n",
        "from google.colab import drive\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from copy import deepcopy\r\n",
        "from sklearn.externals import joblib\r\n",
        "import seaborn as sns\r\n",
        "sns.set(color_codes=True)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from numpy.random import seed\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, Reshape, \\\r\n",
        "          RepeatVector, MaxPooling1D, Conv1D, Flatten, Conv1DTranspose, UpSampling1D, \\\r\n",
        "          AveragePooling1D\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras import regularizers\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "# print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXQT6VF9fRAK"
      },
      "source": [
        "drive.mount(\"/content/drive\")\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6YB2XBaXlTy"
      },
      "source": [
        "# set random seed\r\n",
        "seed(10)\r\n",
        "tf.compat.v1.set_random_seed(10)\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeTPxR0aYNxV"
      },
      "source": [
        "## Load & Process Data\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BInAGYqORGnb"
      },
      "source": [
        "##### Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlN5H_lIhPin"
      },
      "source": [
        "To begin with, we load the 3 parts of the dataset, training, test and validation that we split in the pre-processing phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqlAto997Wp"
      },
      "source": [
        "def load_datasets():\r\n",
        "  x_train = np.asarray(np.load(dirpath + 'x_train.npy')).astype(np.float32)\r\n",
        "  x_test = np.asarray(np.load(dirpath + 'x_test.npy', allow_pickle=True)).astype(np.float32)\r\n",
        "  x_valid = np.asarray(np.load(dirpath + 'x_valid.npy', allow_pickle=True)).astype(np.float32)\r\n",
        "  return x_train, x_test, x_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn-ErKYzhZZx"
      },
      "source": [
        "x_train_unscaled, x_test_unscaled, x_valid_unscaled  = load_datasets()\r\n",
        "print(\"Data loaded. Shapes:\")\r\n",
        "print(x_train_unscaled.shape, x_test_unscaled.shape, x_valid_unscaled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EporEztRJ5L"
      },
      "source": [
        "##### Plot distributions of unscaled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je6ycNzG3EMw"
      },
      "source": [
        "cols = ['Statistic','F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "def data_summary(dataset, label):\r\n",
        "  \"\"\" \r\n",
        "  input:\r\n",
        "    dataset     the three dimensional input (n_samples, n_timepoints, n_features) \r\n",
        "\r\n",
        "    Prints histograms for the 9 features individually\r\n",
        "  returns: \r\n",
        "    summ_df     pd.DataFrame containing summary statistics for the 9 features.\r\n",
        "  \"\"\"\r\n",
        "  data = dataset.reshape((dataset.shape[0] * dataset.shape[1], dataset.shape[2])) # Reshape to 2D (n_samples*n_timepoints, n_features)\r\n",
        "  \r\n",
        "  # Calculate the summary statistics.\r\n",
        "  min   = data.min(axis=0).reshape(1, data.shape[1])                  # Calculate the minimum over the rows for each column.\r\n",
        "  max   = data.max(axis=0).reshape(1, data.shape[1])                  # Then reshape the result to one row and n_cols=n_features, to make it easier to combine later.\r\n",
        "  mean  = data.mean(axis=0).reshape(1, data.shape[1])\r\n",
        "  var   = data.var(axis=0).reshape(1, data.shape[1])\r\n",
        "  q01   = np.quantile(data, 0.01, axis=0).reshape(1, data.shape[1])\r\n",
        "  q99   = np.quantile(data, 0.99, axis=0).reshape(1, data.shape[1])\r\n",
        "\r\n",
        "  names=np.array([['min','max','mean','var','1st percentile', '99th percentile']]).reshape(6,1) # Create a column of names for the summary stats.\r\n",
        "  stats = np.concatenate((min,max,mean,var,q01,q99), axis=0)          # Combine the summary stats in one array\r\n",
        "\r\n",
        "  summ = np.concatenate((names, np.round(stats, 4)), axis=1)          # Combine the summary stats with their names.\r\n",
        "  summ_df = pd.DataFrame(summ, columns=cols)                          # Create a dataframe and supply the channel names as columns.\r\n",
        "\r\n",
        "  # Plot histograms per channel.\r\n",
        "  fig, axes = plt.subplots(3,3, figsize = (9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "  for i in range(9): # Loop through the channels.\r\n",
        "    axes[i].hist(data[:,i], range= (q01[0,i], q99[0,i]),   density=True)    # Add histogram subplot for the values of that channel.\r\n",
        "    axes[i].title.set_text(cols[i+1])                                       # Add a title with the channel name.\r\n",
        "  fig.suptitle(\"Distribution for each channel (between 1st & 99th percentile) of \" + label + \" dataset\" , size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])                                 # Cut the plot space to make space for the global title.\r\n",
        "\r\n",
        "  return summ_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPhAKBtY04dN"
      },
      "source": [
        "# Plot distributions of each channel.\r\n",
        "unscaled_training_summary = data_summary(x_train_unscaled, \"Training\")\r\n",
        "unscaled_test_summary = data_summary(x_test_unscaled, \"Test\")\r\n",
        "unscaled_valid_summary = data_summary(x_valid_unscaled, \"Validation\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obsst4fTzSK4"
      },
      "source": [
        "### Band-pass filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6M8UO7FRueQ"
      },
      "source": [
        "##### Create the filters and apply across the whole data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMKQR0SzQ47"
      },
      "source": [
        "from scipy import signal\r\n",
        "from copy import deepcopy\r\n",
        "\r\n",
        "low_cut = 0.5\r\n",
        "high_cut = 33.0\r\n",
        "\r\n",
        "bp = signal.butter(10, (low_cut,high_cut), 'bp', fs=500, output='sos') # Create the filter. fs is the sampling rate.\r\n",
        "\r\n",
        "# Create copies of the data\r\n",
        "x_train_filtered = deepcopy(x_train_unscaled)         # After running once in the session, I comment these out because otherwise if you re-run the cell it eats RAM.\r\n",
        "x_test_filtered = deepcopy(x_test_unscaled)\r\n",
        "x_valid_filtered = deepcopy(x_valid_unscaled)\r\n",
        "\r\n",
        "print(x_train_filtered.shape)\r\n",
        "\r\n",
        "x_train_filtered = signal.sosfilt(bp, x_train_filtered, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOK0DnGQR95A"
      },
      "source": [
        "##### Plot distributions of the total dataset after the band-pass filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_WtSqx15NJ"
      },
      "source": [
        "data_summary(x_train_filtered, \"Training Filtered\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IW7QFavd7M"
      },
      "source": [
        "### Smooth out extreme points beyond the 1st-99th percentile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbOssMQHviNv"
      },
      "source": [
        "# Reshape to 2 dimensional array\r\n",
        "x_train = deepcopy(x_train_unscaled)\r\n",
        "x_test = deepcopy(x_test_unscaled)\r\n",
        "x_valid = deepcopy(x_valid_unscaled)\r\n",
        "\r\n",
        "n_samples, n_timepoints, n_features = x_train.shape\r\n",
        "n_samples_test =  x_test.shape[0]\r\n",
        "n_samples_valid = x_valid.shape[0]\r\n",
        "\r\n",
        "x_train = x_train.reshape((n_samples*n_timepoints, n_features))\r\n",
        "x_test = x_test.reshape((n_samples_test*n_timepoints, n_features))\r\n",
        "x_valid = x_valid.reshape((n_samples_valid*n_timepoints, n_features))\r\n",
        "\r\n",
        "\r\n",
        "# Find the 1st & 99th percentiles for each column of the training data.\r\n",
        "q01  = np.quantile(x_train, 0.01, axis=0)\r\n",
        "q99  = np.quantile(x_train, 0.99, axis=0)\r\n",
        "\r\n",
        "# Loop through the columns and apply the cutoff\r\n",
        "for i in range(x_train.shape[1]):\r\n",
        "  x_train[x_train[:,i] < q01[i], i] = q01[i] # If the value is below the 1st percentile, replace with the 1st percentile.\r\n",
        "  x_train[x_train[:,i] > q99[i], i] = q99[i] # If the value is above the 99th percentile, replace with the 99th percentile.\r\n",
        "  # Do the same with the test and validation data, using the cutoffs calculated from the training data.\r\n",
        "  x_test[x_test[:,i] < q01[i], i] = q01[i] \r\n",
        "  x_test[x_test[:,i] > q99[i], i] = q99[i] \r\n",
        "  x_valid[x_valid[:,i] < q01[i], i] = q01[i] \r\n",
        "  x_valid[x_valid[:,i] > q99[i], i] = q99[i] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpn8rNubT0LF"
      },
      "source": [
        "### Scale the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6O7PMgjVYEy"
      },
      "source": [
        "##### Scale the artificially smoothed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzi-3d-CxyKt"
      },
      "source": [
        "# normalize the data\r\n",
        "scaler = MinMaxScaler()\r\n",
        "\r\n",
        "x_train = scaler.fit_transform(x_train)\r\n",
        "x_test = scaler.transform(x_test)\r\n",
        "x_valid = scaler.transform(x_valid)\r\n",
        "scaler_filename = \"/content/drive/MyDrive/ml2-eeg-biometrics/scaler_data\"\r\n",
        "joblib.dump(scaler, scaler_filename)\r\n",
        "\r\n",
        "# Reshape to 3D for the convolutional autoencoder\r\n",
        "x_train = x_train.reshape((n_samples, n_timepoints, n_features))\r\n",
        "x_test = x_test.reshape((n_samples_test, n_timepoints, n_features))\r\n",
        "x_valid = x_valid.reshape((n_samples_valid, n_timepoints, n_features))\r\n",
        "print(\"x_train shape: \", x_train.shape, \"x_test shape:\", x_test.shape, \"x_valid shape:\", x_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGo8tjJwVVjZ"
      },
      "source": [
        "##### Scale the filtered data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fvTtfV8evvj"
      },
      "source": [
        "# Scaling applied to the filtered signal.\r\n",
        "scaler_filtered = MinMaxScaler()\r\n",
        "\r\n",
        "n_samples, n_timepoints, n_features = x_train_filtered.shape\r\n",
        "n_samples_test =  x_test_filtered.shape[0]\r\n",
        "\r\n",
        "# Reshape to 2D for the scaler.\r\n",
        "x_train_filtered = x_train_filtered.reshape((n_samples*n_timepoints, n_features))\r\n",
        "# x_test_filtered = x_test_filtered.reshape((n_samples_test*n_timepoints, n_features))\r\n",
        "\r\n",
        "# Apply the scaling.\r\n",
        "x_train_filtered = scaler_filtered.fit_transform(x_train_filtered)\r\n",
        "# x_test_filtered = scaler_filtered.transform(x_test_filtered)\r\n",
        "\r\n",
        "# Re-shape back to 3D for the convolutional autoencoder.\r\n",
        "x_train_filtered = x_train_filtered.reshape((n_samples, n_timepoints, n_features))    \r\n",
        "# x_test_filtered = x_test_filtered.reshape((n_samples_test, n_timepoints, n_features))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Xfm82dUNWF"
      },
      "source": [
        "# data_summary(x_train)     # Plot the distribution of the scaled data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7XEKeEo8a5"
      },
      "source": [
        "##### Visualise the Raw & Scaled Signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DEghaTsYrFO"
      },
      "source": [
        "labels=[(('F3',0), ('F4',1)), (('FC3',2), ('FC4',3)), (('C3',4), ('Cz',5), ('C4',6)), (('CP3',7), ('CP4',8))]\r\n",
        "colours= ['darkslateblue', 'orange','lightskyblue','brown','darkgreen','darkgrey','bisque','violet','palegreen']\r\n",
        "\r\n",
        "def plot_signals(sample, title=None):\r\n",
        "  fig, axes = plt.subplots(2,2, figsize = (6,6))\r\n",
        "  axes=axes.ravel()\r\n",
        "  plt.suptitle(\"Signals\" if title is None else title, size=16)\r\n",
        "  count=0\r\n",
        "  for label_group in labels:\r\n",
        "    for label, ind in label_group:\r\n",
        "      axes[count].plot(sample[:,ind], label=label,color=colours[ind], alpha=0.8)\r\n",
        "      axes[count].legend()\r\n",
        "    count+=1\r\n",
        "\r\n",
        "# plot_signals(x_train[101], title=\"Scaled Signals - x_train[0]\")\r\n",
        "# plot_signals(x_train_unscaled[101], title=\"Unscaled Signals - x_train[0]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PWDBiuWVX8"
      },
      "source": [
        "## Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUsOKCEyxv1"
      },
      "source": [
        "#### Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltaV2CiYXS5J"
      },
      "source": [
        "##### Define a convolutional autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4dPRBsVy2Ot"
      },
      "source": [
        "# Testing around with adding or removing certain layers, no huge difference in performance vs. just using a simpler architecture outlined a few cells down.\r\n",
        "def conv_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(X.shape[1],X.shape[2]))\r\n",
        "    e1 = Conv1D(18, 9, activation='relu', padding='same')(inputs)\r\n",
        "    e2 = MaxPooling1D(2)(e1)\r\n",
        "    e3 = Conv1D(6,5, activation = 'relu', padding='same')(e2)\r\n",
        "    e4 = MaxPooling1D(5)(e3)\r\n",
        "    e5 = Conv1D(2,3, activation = 'relu', padding='same')(e4)\r\n",
        "    e6 = MaxPooling1D(2)(e5)\r\n",
        "    encoded = Flatten()(e6)\r\n",
        "    # encoded = Dense(100, activation='relu')(e5)\r\n",
        "    # d1 = Dense(100, activation='relu')(encoded)\r\n",
        "    d0 = Reshape((125,2))(encoded)\r\n",
        "    d1 = UpSampling1D(2)(d0)\r\n",
        "    d2 = Conv1DTranspose(6, 5, activation='relu', padding='same')(d1)    \r\n",
        "    d3 = UpSampling1D(5)(d2)\r\n",
        "    d4 = Conv1DTranspose(18, 5, activation='relu', padding='same')(d3)\r\n",
        "    d5 = UpSampling1D(2)(d4)\r\n",
        "    d6 = Conv1DTranspose(9, 9, activation='relu', padding='same')(d5)\r\n",
        "    # decoded = Dense(500, activation='sigmoid')(d6)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=d6)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge5xe61zy4AL"
      },
      "source": [
        "# create the autoencoder model\r\n",
        "# model = conv_autoencoder_model(x_train_1f)\r\n",
        "model = conv_autoencoder_model(x_train[:,:,:])\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjpduhZRc2TF"
      },
      "source": [
        "\r\n",
        "class ConvAutoencoder():\r\n",
        "  def __init__(self, loss, optimizer, shape):\r\n",
        "      self.input_shape = shape   \r\n",
        "      self.encoder = None    \r\n",
        "      self.autoencoder_model = self.build_model()\r\n",
        "      self.autoencoder_model.compile(loss=loss, optimizer=optimizer)\r\n",
        "      self.autoencoder_model.summary()\r\n",
        "  \r\n",
        "  ''' Builds the architecture of the model'''   \r\n",
        "  def build_model(self):\r\n",
        "      input_layer = Input(shape=self.input_shape)\r\n",
        "      \r\n",
        "      # encoder\r\n",
        "      x = Conv1D(filters=18, kernel_size=3, activation='relu', padding='same',dilation_rate=2)(input_layer) # When using this layer as the first layer in a model, provide an input_shape argument \r\n",
        "                                                                                                            # (tuple of integers or None, e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, \r\n",
        "                                                                                                            # or (None, 128) for variable-length sequences of 128-dimensional vectors.\r\n",
        "      x1 = MaxPooling1D(pool_size=2)(x) # Downsamples the input representation by taking the maximum value over the window defined by pool_size. The window is shifted by strides. \r\n",
        "                                        # The resulting output when using \"valid\" padding option has a shape of: output_shape = (input_shape - pool_size + 1) / strides)\r\n",
        "      x2 = Conv1D(filters=9,kernel_size=3, activation='relu', padding='same',dilation_rate=2)(x1)\r\n",
        "      x3 = MaxPooling1D(pool_size=2)(x2)\r\n",
        "      x4 = Conv1D(filters=5,kernel_size=3, activation='relu', padding='same',dilation_rate=2)(x3)\r\n",
        "      x5 = MaxPooling1D(pool_size=5)(x4)\r\n",
        "      x6 = Conv1D(filters=2,kernel_size=3, activation='relu', padding='same',dilation_rate=2)(x5)\r\n",
        "      #x7 = AveragePooling1D()(x6)\r\n",
        "      encoded = Flatten()(x6)\r\n",
        "      self.encoder = encoded\r\n",
        "      #encoded = Dense(units=80)(flat)\r\n",
        "\r\n",
        "      # decoder\r\n",
        "      #d1 = Dense(units=150)(encoded) # Densely-connected NN layer.\r\n",
        "      d1 = Reshape((125,2))(encoded) #Layer that reshapes inputs into the given shape.\r\n",
        "      d2 = UpSampling1D(size=2)(d1)\r\n",
        "      d3 = Conv1D(filters=5,kernel_size=3,strides=1, activation='relu', padding='same')(d2)\r\n",
        "      d4 = UpSampling1D(size=5)(d3) #Repeats each temporal step size times along the time axis.\r\n",
        "      d5 = Conv1D(filters=9,kernel_size=3,strides=1, activation='relu', padding='same')(d4)\r\n",
        "      d6 = UpSampling1D(size=2)(d5)\r\n",
        "      decoded = Conv1D(filters=9,kernel_size=3,strides=1, activation='sigmoid', padding='same')(d6)\r\n",
        "\r\n",
        "      model = Model(input_layer, decoded)\r\n",
        "      model.output_shape\r\n",
        "      return model\r\n",
        "\r\n",
        "  ''' Trains the model on the full dataset, requires validation split'''                     \r\n",
        "  def train_model(self, x_train, x_val, epochs, batch_size=20):\r\n",
        "      early_stopping = EarlyStopping(monitor='val_loss',\r\n",
        "                                      min_delta=0,\r\n",
        "                                      patience=5,\r\n",
        "                                      verbose=1, \r\n",
        "                                      mode='auto')\r\n",
        "      history = self.autoencoder_model.fit(x_train, x_train,\r\n",
        "                                            batch_size=batch_size,\r\n",
        "                                            epochs=epochs,\r\n",
        "                                            validation_data=(x_val, x_val),\r\n",
        "                                            callbacks=[early_stopping])\r\n",
        "    \r\n",
        "\r\n",
        "  ''' Trains the model on a single channel, no validation split'''   \r\n",
        "  def train_subset(self, x_train, epochs, batch_size=20):\r\n",
        "      history = self.autoencoder_model.fit(x_train[:,:,0], x_train[:,:,0], epochs=epochs,\r\n",
        "                  batch_size=batch_size,\r\n",
        "                  # validation_split=0.05\r\n",
        "                  )\r\n",
        "      \r\n",
        "  ''' Plots the history'''       \r\n",
        "  def plot_history(self, history):\r\n",
        "      plt.plot(history.history['loss'])\r\n",
        "      plt.plot(history.history['val_loss'])\r\n",
        "      plt.title('Model loss')\r\n",
        "      plt.ylabel('Loss')\r\n",
        "      plt.xlabel('Epoch')\r\n",
        "      plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "      plt.show()\r\n",
        "      \r\n",
        "  ''' Return the predictions for the trained data'''       \r\n",
        "  def eval_model(self, x_test):\r\n",
        "      preds = self.autoencoder_model.predict(x_test)\r\n",
        "      return preds\r\n",
        "\r\n",
        "  ''' Save all model information, including weights, in h5 format'''    \r\n",
        "  def save_model(self, path):\r\n",
        "      timestamp = pd.Timestamp.now()\r\n",
        "      model_name = self.autoencoder_model.name + \"_\" + str(timestamp) + \".h5\"\r\n",
        "      saved_model = self.autoencoder_model.save(path + model_name)\r\n",
        "      print(\"Model saved at \" + str(timestamp))\r\n",
        "\r\n",
        "  def retrieve_encoder(self):\r\n",
        "      encoder = Model(self.autoencoder_model.input, self.autoencoder_model.get_layer(index=8).output)\r\n",
        "      return encoder\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUwCCZ0ZXxdR"
      },
      "source": [
        "##### Fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzzXJTvNyAn0"
      },
      "source": [
        "# this code works for Ricky's implementation\r\n",
        "\r\n",
        "# fit the model to the data\r\n",
        "nb_epochs = 20\r\n",
        "batch_size = 20\r\n",
        "\r\n",
        "x_1 = x_train[:,:,:]#.reshape((-1,2500,4))\r\n",
        "\r\n",
        "history = model.fit(x_1, x_1, epochs=nb_epochs,\r\n",
        "                    batch_size=batch_size,\r\n",
        "                    validation_split=0.05\r\n",
        "                    ).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbFLfZEL_Gdh"
      },
      "source": [
        "# create the simple autoencoder model\r\n",
        "optimizer = Adam(lr=0.001)\r\n",
        "shape = (x_train.shape[1], x_train.shape[2])\r\n",
        "model = ConvAutoencoder(loss='mae', optimizer=optimizer, shape=shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDgOztHOHazh"
      },
      "source": [
        "# this code works for Michele's implementation\r\n",
        "\r\n",
        "# fit the model to the data\r\n",
        "nb_epochs = 2\r\n",
        "batch_size = 20\r\n",
        "x_1 = x_train[:,:,:]\r\n",
        "## Train the model on the full dataset\r\n",
        "model.train_model(x_1, x_valid, nb_epochs, batch_size)\r\n",
        "## Try to train the model on 1 channel but all timepoints after band-pass filter\r\n",
        "#model.train_subset(x_train_1f, x_train_1f, nb_epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64K4Tw5CcXZs"
      },
      "source": [
        "# Obtain the encoder part of the model.\r\n",
        "encoder = model.retrieve_encoder()\r\n",
        "\r\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGDjFwgX8pZ"
      },
      "source": [
        "##### Plot the reconstruction for individual samples.\r\n",
        "Temporary code for assessing the models on the subset of data (one channel and only 20% of timepoints)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hm2DWhiSmeV"
      },
      "source": [
        "x_pred = model.predict(x_test)   # TODO remove obsolete call\r\n",
        "x_pred = model.eval_model(x_1)\r\n",
        "\r\n",
        "for i in range(0,len(x_1),200):\r\n",
        "  plt.plot(x_1[i,:,0], label='actual')\r\n",
        "  plt.plot(x_pred[i,:,0], label = 'predicted')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbHkzSliPCa"
      },
      "source": [
        "# Plot samples.\r\n",
        "# x_pred = model.predict(x_train_filtered[:,:,0])\r\n",
        "X_pred = model.predict(x_test)   # TODO remove obsolete call\r\n",
        "x_pred = model.eval_model(x_1)\r\n",
        "\r\n",
        "\r\n",
        "# Plot actuals vs. prediction for one column across all rows\r\n",
        "# plt.figure(figsize=(14,8))\r\n",
        "# plt.scatter(x_train_filtered[:,0,3], x_pred[:,0,0],alpha=0.2)\r\n",
        "# plt.show()\r\n",
        "\r\n",
        "# # Plot actuals vs. predictions for one row across all columns\r\n",
        "# plt.figure(figsize=(14,8))\r\n",
        "# plt.plot(x_train_filtered[0,:,3], label='actual', alpha=0.7)\r\n",
        "# plt.plot(x_pred[0,:,0], label= 'predicted',alpha=0.7)\r\n",
        "# plt.legend()\r\n",
        "\r\n",
        "# # Plot same as above for a different row\r\n",
        "# plt.figure(figsize=(14,8))\r\n",
        "# plt.plot(x_train_filtered[1039,:,3], label='actual', alpha=0.7)\r\n",
        "# plt.plot(x_pred[1039,:,0], label= 'predicted',alpha=0.7)\r\n",
        "# plt.legend()\r\n",
        "\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_1[1039,:,0], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[1039,:,0], label= 'predicted',alpha=0.7)\r\n",
        "# plt.ylim(0.475, 0.495)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfIG-Na0Rim"
      },
      "source": [
        "\r\n",
        "## Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AvXQtdVLPT"
      },
      "source": [
        "# Get the predicted values for the training set.\r\n",
        "X_pred = model.predict(x_test)   # TODO remove obsolete call\r\n",
        "X_pred_3D = model.eval_model(x_train)\r\n",
        "X_pred = X_pred_3D.reshape(X_pred_3D.shape[0]*X_pred_3D.shape[1], X_pred_3D.shape[2])\r\n",
        "# X_pred = pd.DataFrame(X_pred, columns=train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIl53WipVnBA"
      },
      "source": [
        "##### Plot distribution of the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERj4N100Maz"
      },
      "source": [
        "# Plot the distribution of the loss\r\n",
        "x_train_reshaped = x_train.reshape(x_train.shape[0]*x_train.shape[1], x_train.shape[2])\r\n",
        "loss_mae = np.mean(np.abs(X_pred-x_train_reshaped), axis = 1)\r\n",
        "plt.figure(figsize=(16,9), dpi=80)\r\n",
        "plt.title('Loss Distribution', fontsize=16)\r\n",
        "sns.distplot(loss_mae, bins = 20, kde= True, color = 'blue');\r\n",
        "plt.xlim([0.0,.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uQFT7_1Vtt1"
      },
      "source": [
        "##### Evaluate total re-construction for one sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er4v0JqLR-0M"
      },
      "source": [
        "cols = ['F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "\r\n",
        "def evaluate_prediction(actuals, pred, ind, rescale=False):\r\n",
        "  \"\"\" Function to plot predictions vs. the actuals for one sample.\r\n",
        "  input:\r\n",
        "    actuals   3D array (n_samplesx2500x9) - Original scaled signals.\r\n",
        "    pred      the predicted values corresponding to the actuals.\r\n",
        "    ind     The row number of the sample (2500x9) that you want to compare.\r\n",
        "    rescale   If set to true then the data is first converted back to the original scale for comparison.\r\n",
        "  \r\n",
        "  returns:\r\n",
        "    nothing\r\n",
        "\r\n",
        "  prints plots.\r\n",
        "  \"\"\"\r\n",
        "  if rescale:\r\n",
        "    sample_actual = scaler.inverse_transform(actuals[ind]) # Rescale to the original scale.\r\n",
        "    sample_pred = scaler.inverse_transform(pred[ind]) \r\n",
        "  else:\r\n",
        "    sample_actual = actuals[ind]    # Get the relevant sample.\r\n",
        "    sample_pred = pred[ind]\r\n",
        "\r\n",
        "  mae_by_channel = np.mean(np.abs(sample_pred - sample_actual), axis=0) # Get the Mean Absolute Error for each channel for this sample\r\n",
        "  sample_mae = np.mean(mae_by_channel) # Get the total MAE for the sample by taking the average across the 9 channels\r\n",
        "  print(\"Sample\", ind, \"\\n   Total Mean Absolute Error:\", round(sample_mae, 8))\r\n",
        "  print(\"Mean Absolute Error by Channel:\")\r\n",
        "  for col, error in zip(cols, mae_by_channel):\r\n",
        "    print(col, \": \", round(error,8)) \r\n",
        "\r\n",
        "  fig, axes = plt.subplots(3,3, figsize=(9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "\r\n",
        "  for i in range(9):\r\n",
        "    axes[i].plot(sample_actual[:,i], label= \"Actual\")\r\n",
        "    axes[i].plot(sample_pred[:,i], label=\"Predicted\")\r\n",
        "    axes[i].title.set_text(cols[i] + str(round(mae_by_channel[i], 3)))\r\n",
        "  \r\n",
        "  plt.legend()\r\n",
        "  fig.suptitle(\"Predictions vs. Actuals - Sample \" + str(ind),size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYRpNQjWF-u"
      },
      "source": [
        "evaluate_prediction(x_train, X_pred_3D, ind=1, rescale=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ehGjGxUV4Vf"
      },
      "source": [
        "##### Plot the loss distribution of the test set (code from tutorial, needs to be edited)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2FB1JADjIE"
      },
      "source": [
        "# plot the loss distribution of the test set\r\n",
        "X_pred = model.predict(x_test)   # TODO remove obsolete call\r\n",
        "X_pred = model.eval_model(x_test)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0]*X_pred.shape[1], X_pred.shape[2])\r\n",
        "\r\n",
        "x_test_reshaped = x_test.reshape(x_test.shape[0]*x_test.shape[1], x_test.shape[2])\r\n",
        "fig, axes = plt.subplots(9,1, figsize=(18,9))\r\n",
        "# Plot the loss distribution for each channel individually\r\n",
        "for i in range(x_test_reshaped.shape[1]):\r\n",
        "  loss_mae = np.abs(X_pred[:,i]-x_test_reshaped[:,i])\r\n",
        "  sns.distplot(loss_mae, bins = 100, kde= True, color = 'blue', ax=axes[i]);\r\n",
        "  axes[i].axis(xmin=0.0,xmax=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVIfqljx0lwt"
      },
      "source": [
        "# save all model information, including weights, in h5 format\r\n",
        "model.save_model(\"/content/drive/MyDrive/ml2-eeg-biometrics/\") # Get current timestamp and model number.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}