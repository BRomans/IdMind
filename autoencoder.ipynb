{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQFvwJpfLSt"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXQT6VF9fRAK"
      },
      "source": [
        "drive.mount(\"/content/drive\")\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqlAto997Wp"
      },
      "source": [
        "x_train = np.load(dirpath + 'x_train.npy')\r\n",
        "y_train = np.load(dirpath + 'y_train.npy')\r\n",
        "\r\n",
        "x_test = np.load(dirpath + 'x_test.npy', allow_pickle=True)\r\n",
        "\r\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXIcvQOWxGeW"
      },
      "source": [
        "#Bearing Failure Anomaly Detection\r\n",
        "In this workbook, we use an autoencoder neural network to identify vibrational anomalies from sensor readings in a set of bearings. The goal is to be able to predict future bearing failures before they happen. The vibrational sensor readings are from the NASA Acoustics and Vibration Database. Each data set consists of individual files that are 1-second vibration signal snapshots recorded at 10 minute intervals. Each file contains 20,480 sensor data points that were obtained by reading the bearing sensors at a sampling rate of 20 kHz.\r\n",
        "\r\n",
        "This autoencoder neural network model is created using Long Short-Term Memory (LSTM) recurrent neural network (RNN) cells within the Keras / TensorFlow framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2KjRKO3w2Vk"
      },
      "source": [
        "# import libraries\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.externals import joblib\r\n",
        "import seaborn as sns\r\n",
        "sns.set(color_codes=True)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from numpy.random import seed\r\n",
        "# from tensorflow import set_random_seed\r\n",
        "import tensorflow as tf\r\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\r\n",
        "\r\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\r\n",
        "from keras.models import Model\r\n",
        "from keras import regularizers\r\n",
        "\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy8BoXaHw8Tr"
      },
      "source": [
        "# set random seed\r\n",
        "seed(10)\r\n",
        "# set_random_seed(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaoK5WOUxLWk"
      },
      "source": [
        "### Data loading/processing (unnecessary)\r\n",
        "An assumption is that mechanical degradation in the bearings occurs gradually over time; therefore, we use one datapoint every 10 minutes in the analysis. Each 10 minute datapoint is aggregated by using the mean absolute value of the vibration recordings over the 20,480 datapoints in each file. We then merge together everything in a single dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MonWS_15xR2n"
      },
      "source": [
        "data_dir = 'data/bearing_data'\r\n",
        "merged_data = pd.DataFrame()\r\n",
        "\r\n",
        "for filename in os.listdir(data_dir):\r\n",
        "    dataset = pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\r\n",
        "    dataset_mean_abs = np.array(dataset.abs().mean())\r\n",
        "    dataset_mean_abs = pd.DataFrame(dataset_mean_abs.reshape(1,4))\r\n",
        "    dataset_mean_abs.index = [filename]\r\n",
        "    merged_data = merged_data.append(dataset_mean_abs)\r\n",
        "    \r\n",
        "merged_data.columns = ['Bearing 1', 'Bearing 2', 'Bearing 3', 'Bearing 4']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnjIX0PbxVRO"
      },
      "source": [
        "# transform data file index to datetime and sort in chronological order\r\n",
        "merged_data.index = pd.to_datetime(merged_data.index, format='%Y.%m.%d.%H.%M.%S')\r\n",
        "merged_data = merged_data.sort_index()\r\n",
        "merged_data.to_csv('Averaged_BearingTest_Dataset.csv')\r\n",
        "print(\"Dataset shape:\", merged_data.shape)\r\n",
        "merged_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu_TDcD_xhFs"
      },
      "source": [
        "train = merged_data['2004-02-12 10:52:39': '2004-02-15 12:52:39']\r\n",
        "test = merged_data['2004-02-15 12:52:39':]\r\n",
        "print(\"Training dataset shape:\", train.shape)\r\n",
        "print(\"Test dataset shape:\", test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOPVrNrtxjiz"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(train['Bearing 1'], label='Bearing 1', color='blue', animated = True, linewidth=1)\r\n",
        "ax.plot(train['Bearing 2'], label='Bearing 2', color='red', animated = True, linewidth=1)\r\n",
        "ax.plot(train['Bearing 3'], label='Bearing 3', color='green', animated = True, linewidth=1)\r\n",
        "ax.plot(train['Bearing 4'], label='Bearing 4', color='black', animated = True, linewidth=1)\r\n",
        "plt.legend(loc='lower left')\r\n",
        "ax.set_title('Bearing Sensor Training Data', fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naUVTsCdxn0b"
      },
      "source": [
        "# transforming data from the time domain to the frequency domain using fast Fourier transform\r\n",
        "train_fft = np.fft.fft(train)\r\n",
        "test_fft = np.fft.fft(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8inADg4kxoSS"
      },
      "source": [
        "# frequencies of the healthy sensor signal\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(train_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\r\n",
        "ax.plot(train_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\r\n",
        "ax.plot(train_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\r\n",
        "ax.plot(train_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\r\n",
        "plt.legend(loc='lower left')\r\n",
        "ax.set_title('Bearing Sensor Training Frequency Data', fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XXHSgcZxqGQ"
      },
      "source": [
        "# frequencies of the degrading sensor signal\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(test_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\r\n",
        "ax.plot(test_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\r\n",
        "ax.plot(test_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\r\n",
        "ax.plot(test_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\r\n",
        "plt.legend(loc='lower left')\r\n",
        "ax.set_title('Bearing Sensor Test Frequency Data', fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeTPxR0aYNxV"
      },
      "source": [
        "# Adjusted for EEG Biometrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzi-3d-CxyKt"
      },
      "source": [
        "# normalize the data\r\n",
        "scaler = MinMaxScaler()\r\n",
        "x_train=x_train.reshape((6665*2500,9))\r\n",
        "x_test=x_test.reshape((419*2500,9))\r\n",
        "\r\n",
        "x_train = scaler.fit_transform(x_train)\r\n",
        "x_test = scaler.transform(x_test)\r\n",
        "scaler_filename = \"/content/drive/MyDrive/ml2-eeg-biometrics/scaler_data\"\r\n",
        "joblib.dump(scaler, scaler_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je4mpLs1xysD"
      },
      "source": [
        "# reshape inputs for LSTM [samples, timesteps, features]\r\n",
        "x_train = x_train.reshape((6665,2500,9))\r\n",
        "x_test = x_test.reshape((419,2500,9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWaAMdQtx5FA"
      },
      "source": [
        "# define the autoencoder network model\r\n",
        "def autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(X.shape[1], X.shape[2]))\r\n",
        "    L1 = LSTM(18, activation='tanh', return_sequences=True, \r\n",
        "              kernel_regularizer=regularizers.l2(0.00))(inputs)\r\n",
        "    L2 = LSTM(4, activation='tanh', return_sequences=False)(L1)\r\n",
        "    L3 = RepeatVector(X.shape[1])(L2)\r\n",
        "    L4 = LSTM(4, activation='tanh', return_sequences=True)(L3)\r\n",
        "    L5 = LSTM(18, activation='tanh', return_sequences=True)(L4)\r\n",
        "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \r\n",
        "    model = Model(inputs=inputs, outputs=output)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJjIzolOx8xV"
      },
      "source": [
        "# create the autoencoder model\r\n",
        "model = autoencoder_model(x_train)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzzXJTvNyAn0"
      },
      "source": [
        "# fit the model to the data\r\n",
        "nb_epochs = 5\r\n",
        "batch_size = 10\r\n",
        "history = model.fit(x_train, x_train, epochs=nb_epochs, batch_size=batch_size,\r\n",
        "                    validation_split=0.05).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3LzxyayEBP"
      },
      "source": [
        "# plot the training losses\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\r\n",
        "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\r\n",
        "ax.set_title('Model loss', fontsize=16)\r\n",
        "ax.set_ylabel('Loss (mae)')\r\n",
        "ax.set_xlabel('Epoch')\r\n",
        "ax.legend(loc='upper right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfIG-Na0Rim"
      },
      "source": [
        "\r\n",
        "#Distribution of Loss Function\r\n",
        "By plotting the distribution of the calculated loss in the training set, one can use this to identify a suitable threshold value for identifying an anomaly. In doing this, one can make sure that this threshold is set above the “noise level” and that any flagged anomalies should be statistically significant above the background noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERj4N100Maz"
      },
      "source": [
        "# plot the loss distribution of the training set\r\n",
        "X_pred = model.predict(x_train)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0]*X_pred.shape[1], X_pred.shape[2])\r\n",
        "# X_pred = pd.DataFrame(X_pred, columns=train.columns)\r\n",
        "# X_pred.index = train.index\r\n",
        "\r\n",
        "# scored = pd.DataFrame(index=train.index)\r\n",
        "x_train_reshaped = x_train.reshape(x_train.shape[0]*x_train.shape[1], x_train.shape[2])\r\n",
        "loss_mae = np.mean(np.abs(X_pred-x_train_reshaped), axis = 1)\r\n",
        "plt.figure(figsize=(16,9), dpi=80)\r\n",
        "plt.title('Loss Distribution', fontsize=16)\r\n",
        "sns.distplot(loss_mae, bins = 20, kde= True, color = 'blue');\r\n",
        "plt.xlim([0.0,.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayGk2YGllXya"
      },
      "source": [
        "loss_mae.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVboI1U00c9h"
      },
      "source": [
        "# calculate the loss on the test set\r\n",
        "X_pred = model.predict(X_test)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\r\n",
        "X_pred = pd.DataFrame(X_pred, columns=test.columns)\r\n",
        "X_pred.index = test.index\r\n",
        "\r\n",
        "scored = pd.DataFrame(index=test.index)\r\n",
        "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\r\n",
        "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\r\n",
        "scored['Threshold'] = 0.275\r\n",
        "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\r\n",
        "scored.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1sLxu6V0frb"
      },
      "source": [
        "# calculate the same metrics for the training set \r\n",
        "# and merge all data in a single dataframe for plotting\r\n",
        "X_pred_train = model.predict(X_train)\r\n",
        "X_pred_train = X_pred_train.reshape(X_pred_train.shape[0], X_pred_train.shape[2])\r\n",
        "X_pred_train = pd.DataFrame(X_pred_train, columns=train.columns)\r\n",
        "X_pred_train.index = train.index\r\n",
        "\r\n",
        "scored_train = pd.DataFrame(index=train.index)\r\n",
        "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-Xtrain), axis = 1)\r\n",
        "scored_train['Threshold'] = 0.275\r\n",
        "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\r\n",
        "scored = pd.concat([scored_train, scored])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WwrSeZ30i7v"
      },
      "source": [
        "# plot bearing failure time plot\r\n",
        "scored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVIfqljx0lwt"
      },
      "source": [
        "\r\n",
        "# save all model information, including weights, in h5 format\r\n",
        "model.save(\"Cloud_model.h5\")\r\n",
        "print(\"Model saved\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}