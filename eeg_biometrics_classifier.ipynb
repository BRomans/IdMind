{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg_biometrics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/eeg_biometrics_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkWW2ddS9Tqb"
      },
      "source": [
        "# EEG Biometrics\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCJoISV69Ugz"
      },
      "source": [
        "# Run this cell to load required libraries and mount your Drive folder\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "from sklearn.svm import SVC\r\n",
        "import pandas as pd\r\n",
        "import itertools\r\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRj8JehArfUA"
      },
      "source": [
        "# Seed value\r\n",
        "seed_value = 10\r\n",
        "\r\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "\r\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\r\n",
        "random.seed(seed_value)\r\n",
        "\r\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\r\n",
        "np.random.seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZMivT-Ps4v",
        "outputId": "272a6c93-3595-4f5e-e2d6-009ba09e2709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive')\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJYj9_AoPIv"
      },
      "source": [
        "\r\n",
        "class pairwiseSVM:\r\n",
        "  \"\"\"\r\n",
        "  Define the SVM class which will handle the pairwise manipulation, training & prediction\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, C=1.0, kernel='rbf', degree=3, random_state=None):\r\n",
        "    self.svm = SVC(C=C, kernel=kernel, degree=degree, random_state=random_state)\r\n",
        "\r\n",
        "  def read_train_data(filename, label_col = 'labels', id_col = 'id'):\r\n",
        "    \"\"\"\r\n",
        "    The training data may be read from file or supplied as a DF when fitting the classifier. \r\n",
        "    In both cases, it is assumed that is in DF format with column names, and includes class labels and trial IDs.\r\n",
        "    \"\"\"\r\n",
        "    data = pd.read_csv(filename)\r\n",
        "\r\n",
        "    # Separate the trial IDs, class labels and feature data into separate numpy arrays.\r\n",
        "    train_id = np.array(data[id_col]) # Retrieve the trial IDs for each row & convert to numpy array.\r\n",
        "    y_train = np.array(data[label_col]) # Retrieve the class labels for each row & convert to numpy array.\r\n",
        "    # Retrieve the training features only and convert to numpy array.\r\n",
        "    x_train = data.drop([label_col, id_col], axis=1)\r\n",
        "    x_train = np.array(x_train)\r\n",
        "\r\n",
        "    self.train_id = train_id\r\n",
        "    self.y_train = y_train\r\n",
        "    self.x_train = x_train\r\n",
        "\r\n",
        "  def construct_pairs(self, x_train=None, y_train=None, x_test=None, y_test=None):\r\n",
        "    \"\"\"\r\n",
        "    Method for constructing pairs from the training or testing data.\r\n",
        "    \"\"\"\r\n",
        "    if (x_train is None) ^ (y_train is None):\r\n",
        "      raise Exception(\"Both x_train and y_train datasets should be supplied, or neither.\")\r\n",
        "    elif x_train is None and y_train is None:\r\n",
        "      x_train = self.x_train\r\n",
        "      y_train = self.y_train\r\n",
        "\r\n",
        "    # If x_test is not supplied, we want to construct all pairs of the training data with itself.\r\n",
        "    if x_test is None:\r\n",
        "      # Using the permutations function allows us to get symmetric pairs but excludes pairs of the same index. i.e. both (i,j) and (j,i) will be included but only where i!=j\r\n",
        "      index_pairs = itertools.permutations(range(len(x_train)), 2) # Get all two-way permutations of the indexes.\r\n",
        "\r\n",
        "      n_pairs = len(x_train)*len(x_train) - len(x_train) # All two-way combinations except where the indexes are the same.\r\n",
        "      x_pairs = np.zeros((n_pairs, x_train.shape[1]*2)) # Create a blank array to hold the concatenated feature vector pairs.\r\n",
        "      y_pairs = np.zeros(n_pairs, dtype=np.int8) # Create a blank vector to hold class similarity flag.\r\n",
        "      training_label = np.zeros(n_pairs, dtype=np.int8)\r\n",
        "\r\n",
        "      for count, (i,j) in enumerate(index_pairs):\r\n",
        "        x_pairs[count] = np.concatenate((x_train[i],x_train[j])) # Concatenate the feature vectors for each pair.\r\n",
        "        y_pairs[count] = y_train[i] == y_train[j] # Check if the pair comes from the same class or not.\r\n",
        "        training_label[count] = y_train[i] # Record the class label for the element of the pair coming from the training data. \r\n",
        "    \r\n",
        "    # If x_test is supplied, we want to construct all pairs combining the test data and the training data.\r\n",
        "    elif x_test is not None:\r\n",
        "      index_pairs = itertools.product(range(len(x_train)), range(len(x_test))) # Get all two-way permutations of the indexes.\r\n",
        "\r\n",
        "      n_pairs = len(x_train)*len(x_test) # Get the number of pairs.\r\n",
        "      x_pairs = np.zeros((n_pairs, x_train.shape[1]*2)) # Create a blank array to hold the concatenated feature vector pairs.\r\n",
        "      training_label = np.zeros(n_pairs, dtype=np.int8)\r\n",
        "      # If y_test is also supplied (for evaluating classification accuracy for example), \r\n",
        "      #   then we need to check where the class label is the same for each pair of train/test data.\r\n",
        "      if y_test is not None:\r\n",
        "        y_pairs = np.zeros(n_pairs, dtype=np.int8) # Create a blank vector to hold class similarity flag.\r\n",
        "      else: \r\n",
        "        y_pairs = None\r\n",
        "\r\n",
        "      for count, (i,j) in enumerate(index_pairs):\r\n",
        "        x_pairs[count] = np.concatenate((x_train[i],x_test[j])) # Concatenate the feature vectors for each pair.\r\n",
        "        training_label[count] = y_train[i] # Record the class label for the element of the pair coming from the training data. \r\n",
        "        if y_test is not None:\r\n",
        "          y_pairs[count] = y_train[i] == y_test[j] # Check if the pair comes from the same class or not.\r\n",
        "\r\n",
        "    # Return the concatenated feature vectors for each pair, and the binary label whether they are from the same class.\r\n",
        "    return x_pairs, y_pairs, training_label\r\n",
        "\r\n",
        "\r\n",
        "  def fit(self, x_train = None, y_train = None):\r\n",
        "    \"\"\"\r\n",
        "    Method to fit the SVM on the pairwise training data.\r\n",
        "    \"\"\"\r\n",
        "    if (x_train is None) ^ (y_train is None):\r\n",
        "      raise Exception(\"Either both the x_train and y_train datasets should be supplied, or neither.\")\r\n",
        "\r\n",
        "    # Get all pairwise combinations of the training data.\r\n",
        "    elif x_train is None and y_train is None:\r\n",
        "      x_pairs, y_pairs, _ = self.construct_pairs()\r\n",
        "\r\n",
        "    else:\r\n",
        "      self.x_train = x_train\r\n",
        "      self.y_train = y_train\r\n",
        "      x_pairs, y_pairs, _ = self.construct_pairs(x_train, y_train)\r\n",
        "\r\n",
        "    self.svm.fit(x_pairs, y_pairs)\r\n",
        "\r\n",
        "  def predict_pairwise(self, x_test, y_test=None):\r\n",
        "    \"\"\"Predict the pairwise class similarity with the training data given a set of feature data.\"\"\"\r\n",
        "    x_pairs, y_pairs, training_label = self.construct_pairs(x_test=x_test, y_test=y_test)\r\n",
        "\r\n",
        "    # Return the similarity predictions, the ground truth similarities, and the class label of the training data observation used in the pair.\r\n",
        "    return self.svm.predict(x_pairs), y_pairs, training_label\r\n",
        "  \r\n",
        "  def predict_class(self, x_test, y_test=None):\r\n",
        "    \"\"\"Predict class labels given a set of feature data.\"\"\"\r\n",
        "    y_pairs_pred, y_pairs_true, training_label = self.predict_pairwise(x_test, y_test)\r\n",
        "\r\n",
        "    # Implement voting scheme to decide on class label.\r\n",
        "\r\n",
        "  def add_class(self, new_train, new_class):\r\n",
        "    \"\"\" Add new participant for prediction purposes. \"\"\"\r\n",
        "    pass\r\n",
        "\r\n",
        "  def tune_hyperparameters(self, x_validation, y_validation):\r\n",
        "    \"\"\" Optimise the values of C and the degree using the validation set. \"\"\"\r\n",
        "    pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRosD_BcAIB4"
      },
      "source": [
        "psvm = pairwiseSVM(C=10.0, kernel='rbf', degree=3, random_state=None) # Create a test instance of the class.\r\n",
        "\r\n",
        "# # Some small test data\r\n",
        "# a = np.array([[1,2,3],[4,5,6],[7,8,9],[9,10,11],[2,9,10]])\r\n",
        "# b = np.array([[901,801,701],[602,603,604]])\r\n",
        "# y_a = np.array([0,1,1,2,2])\r\n",
        "# y_b = np.array([1,1])\r\n",
        "\r\n",
        "a = np.array([[1],[2.5],[3.0],[3.7],[5.2],[5.8],[7.1],[7.2],[7.4],[10]])\r\n",
        "y_a = np.array([0,1,1,1,2,2,3,3,3,4])\r\n",
        "\r\n",
        "x_pairs, y_pairs, training_label = psvm.construct_pairs(x_train=a,y_train=y_a)\r\n",
        "psvm.fit(a,y_a)\r\n",
        "# psvm.svm.fit(a, y_a) # Test a regular SVM to separate the classes without a pairwise approach.\r\n",
        "psvm.predict_pairwise(a,y_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36i5EbbEYjXq"
      },
      "source": [
        "x = [z[0] for z in x_pairs]\r\n",
        "y = [z[1] for z in x_pairs]\r\n",
        "plt.scatter(x,y, c=y_pairs)\r\n",
        "\r\n",
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\r\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\r\n",
        "    if ax is None:\r\n",
        "        ax = plt.gca()\r\n",
        "    xlim = ax.get_xlim()\r\n",
        "    ylim = ax.get_ylim()\r\n",
        "    \r\n",
        "    # create grid to evaluate model\r\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\r\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\r\n",
        "    Y, X = np.meshgrid(y, x)\r\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\r\n",
        "    P = model.decision_function(xy).reshape(X.shape)\r\n",
        "    \r\n",
        "    # plot decision boundary and margins\r\n",
        "    ax.contour(X, Y, P, colors='k',\r\n",
        "               levels=[-1, 0, 1], alpha=0.5,\r\n",
        "               linestyles=['--', '-', '--'])\r\n",
        "    \r\n",
        "    # plot support vectors\r\n",
        "    if plot_support:\r\n",
        "        ax.scatter(model.support_vectors_[:, 0],\r\n",
        "                   model.support_vectors_[:, 1],\r\n",
        "                   s=300, linewidth=1, facecolors='none');\r\n",
        "    ax.set_xlim(xlim)\r\n",
        "    ax.set_ylim(ylim)\r\n",
        "\r\n",
        "plot_svc_decision_function(psvm.svm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwAnUz3Iyqiy"
      },
      "source": [
        "# Structure to evaluate classification performance\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "def print_results(Y_test, predictions, label_names):\r\n",
        "    print(classification_report(Y_test, predictions))\r\n",
        "    print(\"Classification Accuracy: {0:.3f}\".format(accuracy_score(Y_test, predictions)))\r\n",
        "\r\n",
        "    conf_mat = confusion_matrix(predictions, Y_test)\r\n",
        "\r\n",
        "    fig = plt.figure(figsize=(6,6))\r\n",
        "    width = np.shape(conf_mat)[1]\r\n",
        "    height = np.shape(conf_mat)[0]\r\n",
        "\r\n",
        "    plt.figure(figsize=(12,12))\r\n",
        "    res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\r\n",
        "    for i, row in enumerate(conf_mat):\r\n",
        "        for j, c in enumerate(row):\r\n",
        "            if c>0:\r\n",
        "                plt.text(j-.2, i+.1, c, fontsize=16)\r\n",
        "\r\n",
        "    # cb = fig.colorbar(res)\r\n",
        "    plt.title('Confusion Matrix')\r\n",
        "    _ = plt.xticks(range(6), label_names, rotation=90)\r\n",
        "    _ = plt.yticks(range(6), label_names)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw7uCGxjFH7-"
      },
      "source": [
        "# Run this cell to save the changes\r\n",
        "\r\n",
        "# drive.flush_and_unmount()\r\n",
        "# print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4KOVB7lQD-6"
      },
      "source": [
        "# x_train_1 = np.load(dirpath + 'encoding_model_2021-01-18 20:30:21.496480.npy')\r\n",
        "# x_train_2 = np.load(dirpath + 'cand_encoding_model_2021-01-18 20:30:21.496480.npy')\r\n",
        "\r\n",
        "# x_train_1 = np.load(dirpath + 'train_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "# x_train_2 = np.load(dirpath + 'train_cand_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "\r\n",
        "# x_valid_1 = np.load(dirpath + 'valid_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "# x_valid_2 = np.load(dirpath + 'valid_cand_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "\r\n",
        "x_train = np.load(dirpath + 'train_encoding_model_2021-01-19 19:36:47.206950.npy')\r\n",
        "x_valid = np.load(dirpath + 'valid_encoding_model_2021-01-19 19:36:47.206950.npy')\r\n",
        "\r\n",
        "y_train = np.load(dirpath + 'y_train.npy')\r\n",
        "y_train = y_train.reshape((-1,))\r\n",
        "\r\n",
        "y_valid= np.load(dirpath + 'y_valid.npy', allow_pickle=True)\r\n",
        "y_valid = y_valid.reshape((-1,))\r\n",
        "y_valid = np.array(y_valid, dtype='int64') # Read in as object vector with allow_pickle, not sure why.\r\n",
        "\r\n",
        "id_train = np.load(dirpath + 'id_train.npy', allow_pickle=True)\r\n",
        "id_train = id_train.reshape((-1, 5))\r\n",
        "\r\n",
        "id_valid = np.load(dirpath + 'id_valid.npy', allow_pickle=True)\r\n",
        "id_valid = id_valid.reshape((-1, 5))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgysRyivRSfV"
      },
      "source": [
        "# svm_1 = SVC(C=10.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "# svm_2 = SVC(C=10.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "\r\n",
        "# svm_1.fit(x_train_1, y_train)\r\n",
        "# svm_2.fit(x_train_2, y_train)\r\n",
        "\r\n",
        "# y_pred_1 = svm_1.predict(x_train_1)\r\n",
        "# y_pred_2 = svm_2.predict(x_train_2)\r\n",
        "\r\n",
        "svm = SVC(C=100, kernel='rbf', degree=3, random_state=0)\r\n",
        "svm.fit(x_train, y_train)\r\n",
        "\r\n",
        "train_pred = svm.predict(x_train)\r\n",
        "valid_pred = svm.predict(x_valid)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97yNU3Y7n7cg"
      },
      "source": [
        "valid_pred_1 =  svm_1.predict(x_valid_1)\r\n",
        "valid_pred_2 = svm_2.predict(x_valid_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYd5MHkeY6PT"
      },
      "source": [
        "psvm_1 = pairwiseSVM(C=10.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "# psvm_2 = pairwiseSVM(C=10.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "\r\n",
        "x_subset = x_train_1[::10,:]\r\n",
        "y_subset = y_train[::10]\r\n",
        "\r\n",
        "psvm_1.fit(x_subset, y_subset)\r\n",
        "# psvm_2.fit(x_train_2, y_train)\r\n",
        "\r\n",
        "# y_pred_1p = psvm_1.predict(x_train_1)\r\n",
        "# y_pred_2p = psvm_2.predict(x_train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOW3qJ0wO6sg"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "# parameters = {'kernel':('linear', 'rbf'), 'C':[0.01, 0.1, 1, 10, 100], 'gamma':[0.001, 0.01, 0.1, 1, 10, 100], 'degree':[2,3,4]}\r\n",
        "parameters = {'C':[0.1, 1, 10], 'gamma':[0.0001, 0.01, 0.1]}\r\n",
        "\r\n",
        "clf = GridSearchCV(svm, parameters)\r\n",
        "clf.fit(x_train, y_train)\r\n",
        "\r\n",
        "# Utility function to report best scores\r\n",
        "def report(results, n_top=3):\r\n",
        "    for i in range(1, n_top + 1):\r\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\r\n",
        "        for candidate in candidates:\r\n",
        "            print(\"Model with rank: {0}\".format(i))\r\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\r\n",
        "                  .format(results['mean_test_score'][candidate],\r\n",
        "                          results['std_test_score'][candidate]))\r\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\r\n",
        "            print(\"\")\r\n",
        "\r\n",
        "report(clf.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKV5X4T2SIM8"
      },
      "source": [
        "print_results(y_train, train_pred, np.unique(y_train).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eMNn3olofIu"
      },
      "source": [
        "print_results(y_valid, valid_pred, np.unique(y_valid).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}