{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg_biometrics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/eeg_biometrics_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkWW2ddS9Tqb"
      },
      "source": [
        "# EEG Biometrics\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCJoISV69Ugz"
      },
      "source": [
        "# Run this cell to load required libraries and mount your Drive folder\r\n",
        "import numpy as np\r\n",
        "from numpy.random import choice\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "from sklearn.svm import SVC\r\n",
        "import pandas as pd\r\n",
        "import itertools\r\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRj8JehArfUA"
      },
      "source": [
        "# Seed value\r\n",
        "seed_value = 10\r\n",
        "\r\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\r\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\r\n",
        "\r\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\r\n",
        "random.seed(seed_value)\r\n",
        "\r\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\r\n",
        "np.random.seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ZMivT-Ps4v"
      },
      "source": [
        "drive.mount('/content/drive')\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiki8q0HidL6"
      },
      "source": [
        "# Empty iterator\r\n",
        "def empty_gen(): \r\n",
        "   yield from ()\r\n",
        "  \r\n",
        "# Helper function for constructing pairs.\r\n",
        "def get_index_pairs(y, size='balanced'):\r\n",
        "  iter_same_class, iter_diff_class = empty_gen(), empty_gen()\r\n",
        "\r\n",
        "  total_same, total_diff = 0, 0\r\n",
        "  # Loop over the classes\r\n",
        "  for c in np.unique(y):\r\n",
        "    c_indexes = np.where(y == c)[0]                                            # find the indexes where the class is the same as the current class.\r\n",
        "    non_c_indexes = np.where(y != c)[0]                                        # find the indexes where the class is different from the current class.\r\n",
        "    \r\n",
        "    if size is not None:                                                       # If size is None, then take all different-class indexes.\r\n",
        "      size = int(np.ceil(len(c_indexes)/2)) if size=='balanced' else size      # If the size should be balanced, then we should take as many different-class examples as same-class examples. We'll be taking the pair in reverse order too, so divide by 2 here.\r\n",
        "      non_c_indexes = choice(non_c_indexes, size=size, replace=False)          # Take a random subset of the indexes.\r\n",
        "\r\n",
        "    if len(c_indexes) > 1:\r\n",
        "      iter = itertools.permutations(np.nditer(c_indexes), 2)                     # Get all permutations of same-class indexes.\r\n",
        "      iter_same_class = itertools.chain(iter_same_class, iter)                   # Chain (concatenate) this with the existing indexes.\r\n",
        "\r\n",
        "    if len(non_c_indexes) > 0:\r\n",
        "      iter = itertools.product(np.nditer(c_indexes), np.nditer(non_c_indexes))   # Get permutations of indexes with different classes.\r\n",
        "      iter_diff_class = itertools.chain(iter_diff_class, iter)                   # Chain this with the existing indexes.\r\n",
        "\r\n",
        "    total_same += len(c_indexes)*len(c_indexes) - len(c_indexes)\r\n",
        "    total_diff += len(non_c_indexes)*len(c_indexes)\r\n",
        "  \r\n",
        "  return iter_same_class, iter_diff_class, total_same, total_diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJYj9_AoPIv"
      },
      "source": [
        "\r\n",
        "class pairwiseSVM:\r\n",
        "  \"\"\"\r\n",
        "  Define the SVM class which will handle the pairwise manipulation, training & prediction\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, C=1.0, kernel='rbf', degree=3, random_state=None):\r\n",
        "    self.svm = SVC(C=C, kernel=kernel, degree=degree, random_state=random_state)\r\n",
        "\r\n",
        "  def read_train_data(filename, label_col = 'labels', id_col = 'id'):\r\n",
        "    \"\"\"\r\n",
        "    The training data may be read from file or supplied as a DF when fitting the classifier. \r\n",
        "    In both cases, it is assumed that is in DF format with column names, and includes class labels and trial IDs.\r\n",
        "    \"\"\"\r\n",
        "    data = pd.read_csv(filename)\r\n",
        "\r\n",
        "    # Separate the trial IDs, class labels and feature data into separate numpy arrays.\r\n",
        "    train_id = np.array(data[id_col]) # Retrieve the trial IDs for each row & convert to numpy array.\r\n",
        "    y_train = np.array(data[label_col]) # Retrieve the class labels for each row & convert to numpy array.\r\n",
        "    # Retrieve the training features only and convert to numpy array.\r\n",
        "    x_train = data.drop([label_col, id_col], axis=1)\r\n",
        "    x_train = np.array(x_train)\r\n",
        "\r\n",
        "    self.train_id = train_id\r\n",
        "    self.y_train = y_train\r\n",
        "    self.x_train = x_train\r\n",
        "\r\n",
        "  def construct_pairs(self, x_train=None, y_train=None, x_test=None, y_test=None):\r\n",
        "    \"\"\"\r\n",
        "    Method for constructing pairs from the training or testing data.\r\n",
        "    \"\"\"\r\n",
        "    if (x_train is None) ^ (y_train is None):\r\n",
        "      raise Exception(\"Both x_train and y_train datasets should be supplied, or neither.\")\r\n",
        "    elif x_train is None and y_train is None:\r\n",
        "      x_train = self.x_train\r\n",
        "      y_train = self.y_train\r\n",
        "\r\n",
        "    # If x_test is not supplied, we want to construct all pairs of the training data with itself.\r\n",
        "    if x_test is None:\r\n",
        "      # Using the permutations function allows us to get symmetric pairs but excludes pairs of the same index. i.e. both (i,j) and (j,i) will be included but only where i!=j\r\n",
        "      # index_pairs = itertools.permutations(range(len(x_train)), 2) # Get all two-way permutations of the indexes.\r\n",
        "\r\n",
        "      index_pairs_same, index_pairs_diff, n_same, n_diff = get_index_pairs(y_train, size='balanced')\r\n",
        "    \r\n",
        "      n_pairs = n_same + n_diff*2\r\n",
        "      # n_pairs = len(x_train)*len(x_train) - len(x_train)  # All two-way combinations except where the indexes are the same.\r\n",
        "      x_pairs = np.zeros((n_pairs, x_train.shape[1]*2))   # Create a blank array to hold the concatenated feature vector pairs.\r\n",
        "      y_pairs = np.zeros(n_pairs, dtype=np.int8)          # Create a blank vector to hold class similarity flag.\r\n",
        "      training_label = np.zeros(n_pairs, dtype=np.int8)   # Vector to hold the class label of the training example (left-hand side of the comparison).\r\n",
        "      test_index = np.zeros(n_pairs, dtype=np.uint16)     # Vector to hold the index of the test example, so that we can easily implement the voting scheme for each test example.\r\n",
        "\r\n",
        "      count=0\r\n",
        "      for i, j in index_pairs_same:\r\n",
        "        x_pairs[count] = np.concatenate((x_train[i],x_train[j]))  # Concatenate the feature vectors for each pair.\r\n",
        "        y_pairs[count] = 1                                        # These pairs come from the same class\r\n",
        "        training_label[count] = y_train[i]                        # Record the class label for the element of the pair coming from the training data. \r\n",
        "        test_index[count] = j                                     # Record the index of the test example being used.\r\n",
        "        count += 1                                                # Increment the counter.\r\n",
        "\r\n",
        "      for i, j in index_pairs_diff:\r\n",
        "        x_pairs[count] = np.concatenate((x_train[i],x_train[j]))    # Concatenate the feature vectors for each pair.\r\n",
        "        x_pairs[count+1] = np.concatenate((x_train[j],x_train[i]))  # Concatenate the feature vectors for each pair.\r\n",
        "        y_pairs[count:count+2] = 0                                  # These pairs come from different classes.\r\n",
        "        training_label[count] = y_train[i]                          # Record the class label for the element of the pair coming from the training data. \r\n",
        "        training_label[count+1] = y_train[j]\r\n",
        "        test_index[count] = j                                       # Record the index of the test example being used.\r\n",
        "        test_index[count+1] = i                                             \r\n",
        "        count += 2                                                  # Increment by two since we're adding two pairs per loop.\r\n",
        "\r\n",
        "    \r\n",
        "    # If x_test is supplied, we want to construct all pairs combining the test data and the training data.\r\n",
        "    elif x_test is not None:\r\n",
        "      index_pairs = itertools.product(range(len(x_train)), range(len(x_test))) # Get all two-way permutations of the indexes.\r\n",
        "\r\n",
        "      n_pairs = len(x_train)*len(x_test)                          # Get the number of pairs.\r\n",
        "      x_pairs = np.zeros((n_pairs, x_train.shape[1]*2))           # Create a blank array to hold the concatenated feature vector pairs.\r\n",
        "      training_label = np.zeros(n_pairs, dtype=np.int8)           # Vector to hold the class label of the training example (left-hand side of the comparison).\r\n",
        "      test_index = np.zeros(n_pairs, dtype=np.uint16)             # Vector to hold the index of the test example, so that we can easily implement the voting scheme for each test example.\r\n",
        "\r\n",
        "      # If y_test is also supplied (for evaluating classification accuracy for example), \r\n",
        "      #   then we need to check where the class label is the same for each pair of train/test data.\r\n",
        "      if y_test is not None:\r\n",
        "        y_pairs = np.zeros(n_pairs, dtype=np.int8)                # Create a blank vector to hold class similarity flag.\r\n",
        "      else: \r\n",
        "        y_pairs = None\r\n",
        "\r\n",
        "      for count, (i,j) in enumerate(index_pairs):\r\n",
        "        x_pairs[count] = np.concatenate((x_train[i],x_test[j]))   # Concatenate the feature vectors for each pair.\r\n",
        "        training_label[count] = y_train[i]                        # Record the class label for the element of the pair coming from the training data. \r\n",
        "        test_index[count] = j                                     # Record the index of the test example being used.\r\n",
        "        if y_test is not None:\r\n",
        "          y_pairs[count] = y_train[i] == y_test[j]                # Check if the pair comes from the same class or not.\r\n",
        "\r\n",
        "    # Return the concatenated feature vectors for each pair, and the binary label whether they are from the same class.\r\n",
        "    return x_pairs, y_pairs, training_label, test_index\r\n",
        "\r\n",
        "\r\n",
        "  def fit(self, x_train = None, y_train = None):\r\n",
        "    \"\"\"\r\n",
        "    Method to fit the SVM on the pairwise training data.\r\n",
        "    \"\"\"\r\n",
        "    if (x_train is None) ^ (y_train is None):\r\n",
        "      raise Exception(\"Either both the x_train and y_train datasets should be supplied, or neither.\")\r\n",
        "\r\n",
        "    # Get all pairwise combinations of the training data.\r\n",
        "    elif x_train is None and y_train is None:\r\n",
        "      x_pairs, y_pairs, _,_ = self.construct_pairs()\r\n",
        "\r\n",
        "    else:\r\n",
        "      self.x_train = x_train\r\n",
        "      self.y_train = y_train\r\n",
        "      x_pairs, y_pairs, _,_ = self.construct_pairs(x_train, y_train)\r\n",
        "\r\n",
        "    self.svm.fit(x_pairs, y_pairs)\r\n",
        "\r\n",
        "  def predict_pairwise(self, x_test, y_test=None):\r\n",
        "    \"\"\"Predict the pairwise class similarity with the training data given a set of feature data.\"\"\"\r\n",
        "    x_pairs, y_pairs, training_label, test_index = self.construct_pairs(x_test=x_test, y_test=y_test)\r\n",
        "\r\n",
        "    # Return the similarity predictions, the ground truth similarities, and the class label of the training data observation used in the pair.\r\n",
        "    return self.svm.predict(x_pairs), y_pairs, training_label, test_index\r\n",
        "  \r\n",
        "  def predict_class(self, x_test, y_test=None):\r\n",
        "    \"\"\"Predict class labels given a set of feature data.\"\"\"\r\n",
        "    y_pairs_pred, y_pairs_true, training_label, test_index = self.predict_pairwise(x_test, y_test)\r\n",
        "\r\n",
        "    # Implement voting scheme to decide on class label.\r\n",
        "    df = pd.DataFrame({'label':training_label[y_pairs_pred==1], 'test_index': test_index[y_pairs_pred==1]})\r\n",
        "    df.value_counts(['label','test_index'])\r\n",
        "\r\n",
        "  def add_class(self, new_train, new_class):\r\n",
        "    \"\"\" Add new participant for prediction purposes. \"\"\"\r\n",
        "    pass\r\n",
        "\r\n",
        "  def tune_hyperparameters(self, x_validation, y_validation):\r\n",
        "    \"\"\" Optimise the values of C and the degree using the validation set. \"\"\"\r\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0qHjqfJaCH5"
      },
      "source": [
        "#### Small test for the pairwise SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRosD_BcAIB4"
      },
      "source": [
        "# psvm = pairwiseSVM(C=10.0, kernel='rbf', degree=3, random_state=None) # Create a test instance of the class.\r\n",
        "\r\n",
        "# # # Some small test data\r\n",
        "# # a = np.array([[1,2,3],[4,5,6],[7,8,9],[9,10,11],[2,9,10]])\r\n",
        "# # b = np.array([[901,801,701],[602,603,604]])\r\n",
        "# # y_a = np.array([0,1,1,2,2])\r\n",
        "# # y_b = np.array([1,1])\r\n",
        "\r\n",
        "# a = np.array([[1],[2.5],[3.0],[3.7],[5.2],[5.8],[7.1],[7.2],[7.4],[10]])\r\n",
        "# y_a = np.array([0,1,1,1,2,2,3,3,3,4])\r\n",
        "\r\n",
        "# # x_pairs, y_pairs, training_label,_ = psvm.construct_pairs(x_train=a,y_train=y_a)\r\n",
        "# psvm.fit(a,y_a)\r\n",
        "# # psvm.svm.fit(a, y_a) # Test a regular SVM to separate the classes without a pairwise approach.\r\n",
        "# psvm.predict_pairwise(a,y_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36i5EbbEYjXq"
      },
      "source": [
        "x = [z[0] for z in x_pairs]\r\n",
        "y = [z[1] for z in x_pairs]\r\n",
        "plt.scatter(x,y, c=y_pairs)\r\n",
        "\r\n",
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\r\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\r\n",
        "    if ax is None:\r\n",
        "        ax = plt.gca()\r\n",
        "    xlim = ax.get_xlim()\r\n",
        "    ylim = ax.get_ylim()\r\n",
        "    \r\n",
        "    # create grid to evaluate model\r\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\r\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\r\n",
        "    Y, X = np.meshgrid(y, x)\r\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\r\n",
        "    P = model.decision_function(xy).reshape(X.shape)\r\n",
        "    \r\n",
        "    # plot decision boundary and margins\r\n",
        "    ax.contour(X, Y, P, colors='k',\r\n",
        "               levels=[-1, 0, 1], alpha=0.5,\r\n",
        "               linestyles=['--', '-', '--'])\r\n",
        "    \r\n",
        "    # plot support vectors\r\n",
        "    if plot_support:\r\n",
        "        ax.scatter(model.support_vectors_[:, 0],\r\n",
        "                   model.support_vectors_[:, 1],\r\n",
        "                   s=300, linewidth=1, facecolors='none');\r\n",
        "    ax.set_xlim(xlim)\r\n",
        "    ax.set_ylim(ylim)\r\n",
        "\r\n",
        "plot_svc_decision_function(psvm.svm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwAnUz3Iyqiy"
      },
      "source": [
        "# Structure to evaluate classification performance\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "def print_results(Y_test, predictions, label_names):\r\n",
        "    print(classification_report(Y_test, predictions))\r\n",
        "    print(\"Classification Accuracy: {0:.3f}\".format(accuracy_score(Y_test, predictions)))\r\n",
        "\r\n",
        "    conf_mat = confusion_matrix(Y_test, predictions)\r\n",
        "\r\n",
        "    fig = plt.figure(figsize=(6,6))\r\n",
        "    width = np.shape(conf_mat)[1]\r\n",
        "    height = np.shape(conf_mat)[0]\r\n",
        "\r\n",
        "    plt.figure(figsize=(12,12))\r\n",
        "    res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\r\n",
        "    for i, row in enumerate(conf_mat):\r\n",
        "        for j, c in enumerate(row):\r\n",
        "            if c>0:\r\n",
        "                plt.text(j-.2, i+.1, c, fontsize=16)\r\n",
        "\r\n",
        "    # cb = fig.colorbar(res)\r\n",
        "    plt.title('Confusion Matrix')\r\n",
        "    # _ = plt.xticks(range(6), label_names, rotation=90)\r\n",
        "    # _ = plt.yticks(range(6), label_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw7uCGxjFH7-"
      },
      "source": [
        "# Run this cell to save the changes\r\n",
        "\r\n",
        "# drive.flush_and_unmount()\r\n",
        "# print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4KOVB7lQD-6"
      },
      "source": [
        "# x_train_1 = np.load(dirpath + 'encoding_model_2021-01-18 20:30:21.496480.npy')\r\n",
        "# x_train_2 = np.load(dirpath + 'cand_encoding_model_2021-01-18 20:30:21.496480.npy')\r\n",
        "\r\n",
        "# x_train_1 = np.load(dirpath + 'train_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "# x_train_2 = np.load(dirpath + 'train_cand_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "\r\n",
        "# x_valid_1 = np.load(dirpath + 'valid_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "# x_valid_2 = np.load(dirpath + 'valid_cand_encoding_model_2021-01-18 22:24:31.844829.npy')\r\n",
        "\r\n",
        "x_train = np.load(dirpath + 'train_encoding_model_2021-01-19 19:36:47.206950.npy')\r\n",
        "x_valid = np.load(dirpath + 'valid_encoding_model_2021-01-19 19:36:47.206950.npy')\r\n",
        "\r\n",
        "y_train = np.load(dirpath + 'y_train.npy')\r\n",
        "y_train = y_train.reshape((-1,))\r\n",
        "\r\n",
        "y_valid= np.load(dirpath + 'y_valid.npy', allow_pickle=True)\r\n",
        "y_valid = y_valid.reshape((-1,))\r\n",
        "y_valid = np.array(y_valid, dtype='int64') # Read in as object vector with allow_pickle, not sure why.\r\n",
        "\r\n",
        "id_train = np.load(dirpath + 'id_train.npy', allow_pickle=True)\r\n",
        "id_train = id_train.reshape((-1, 5))\r\n",
        "\r\n",
        "id_valid = np.load(dirpath + 'id_valid.npy', allow_pickle=True)\r\n",
        "id_valid = id_valid.reshape((-1, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgysRyivRSfV"
      },
      "source": [
        "# svm_1 = SVC(C=10.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "# svm_2 = SVC(C=10.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "\r\n",
        "# svm_1.fit(x_train_1, y_train)\r\n",
        "# svm_2.fit(x_train_2, y_train)\r\n",
        "\r\n",
        "# y_pred_1 = svm_1.predict(x_train_1)\r\n",
        "# y_pred_2 = svm_2.predict(x_train_2)\r\n",
        "\r\n",
        "svm = SVC(C=100, kernel='rbf', degree=3, random_state=0)\r\n",
        "svm.fit(x_train, y_train)\r\n",
        "\r\n",
        "train_pred = svm.predict(x_train)\r\n",
        "valid_pred = svm.predict(x_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97yNU3Y7n7cg"
      },
      "source": [
        "valid_pred_1 =  svm_1.predict(x_valid_1)\r\n",
        "valid_pred_2 = svm_2.predict(x_valid_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYd5MHkeY6PT"
      },
      "source": [
        "from time import time\r\n",
        "prev=time()\r\n",
        "\r\n",
        "psvm = pairwiseSVM(C=1.0, kernel='rbf', degree=3, random_state=0)\r\n",
        "\r\n",
        "x_subset = x_train[::10,:]\r\n",
        "y_subset = y_train[::10]\r\n",
        "\r\n",
        "# pairs = psvm.construct_pairs(x_subset, y_subset)\r\n",
        "psvm.fit(x_subset, y_subset)\r\n",
        "\r\n",
        "print(round(time()-prev, 5), \" seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEViOhofhhTn"
      },
      "source": [
        "x_valid_subset = x_valid[::5,:]\r\n",
        "y_valid_subset = y_valid[::5]\r\n",
        "print(x_valid_subset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpbSzYDAVdg-"
      },
      "source": [
        "# y_pairs_pred, y_pairs_true, training_label, test_index = psvm.predict_pairwise(x_subset, y_subset)\r\n",
        "x_pairs, y_pairs_true, training_label, test_index = psvm.construct_pairs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-lTNDzRHge_"
      },
      "source": [
        "prev=time()\r\n",
        "\r\n",
        "y_pairs_pred = psvm.svm.predict(x_pairs)\r\n",
        "\r\n",
        "print(round(time()-prev, 5), \" seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7wyctvxofiY"
      },
      "source": [
        "prev=time()\r\n",
        "\r\n",
        "x_pairs_valid, y_pairs_true_valid, training_label_valid, test_index_valid = psvm.construct_pairs(x_test=x_valid_subset, y_test=y_valid_subset)\r\n",
        "\r\n",
        "print(x_pairs_valid.shape)\r\n",
        "print(round(time()-prev, 5), \" seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQPQsJLzo4Cy"
      },
      "source": [
        "prev=time()\r\n",
        "\r\n",
        "y_pairs_pred_valid = psvm.svm.predict(x_pairs_valid)\r\n",
        "print(y_pairs_pred_valid.shape)\r\n",
        "print(round(time()-prev, 5), \" seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1kg_q-Ddi0"
      },
      "source": [
        "y_pairs_pred_valid, y_pairs_true_valid, training_label_valid, test_index_valid = psvm.predict_pairwise(x_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W01i3p_BvtYy"
      },
      "source": [
        "# Implement voting scheme to decide on class label.\r\n",
        "df = pd.DataFrame({'label':training_label[y_pairs_pred==1], 'test_index': test_index[y_pairs_pred==1]})\r\n",
        "df = df.groupby(['test_index', 'label'], as_index=False).size().sort_values(by='size', ascending=False)            # For each test_index (sample of test data), get the count for each predicted label.\r\n",
        "df = df.drop_duplicates(subset='test_index').sort_values(by='test_index')                                          # One row for each test_index.\r\n",
        "y_class_pred = df.label.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5RkHrEH1zST"
      },
      "source": [
        "df = pd.DataFrame({'label':training_label_valid[y_pairs_pred_valid==1], 'test_index': test_index_valid[y_pairs_pred_valid==1]})\r\n",
        "df = df.groupby(['test_index', 'label'], as_index=False).size().sort_values(by='size', ascending=False)            # For each test_index (sample of test data), get the count for each predicted label.\r\n",
        "df = df.drop_duplicates(subset='test_index').sort_values(by='test_index')                                          # One row for each test_index.\r\n",
        "y_class_pred_valid = df.label.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8YDhl41woBR"
      },
      "source": [
        "print_results(y_subset, y_class_pred, np.unique(y_subset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNz-Onzc18_H"
      },
      "source": [
        "print_results(y_valid_subset, y_class_pred_valid, np.unique(y_valid_subset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pXNA9Cka6sO"
      },
      "source": [
        "print_results(y_pairs_true, y_pairs_pred, label_names=[0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncSq-_vxh8dc"
      },
      "source": [
        "print_results(y_pairs_true_valid, y_pairs_pred_valid, label_names=[0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOW3qJ0wO6sg"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "# parameters = {'kernel':('linear', 'rbf'), 'C':[0.01, 0.1, 1, 10, 100], 'gamma':[0.001, 0.01, 0.1, 1, 10, 100], 'degree':[2,3,4]}\r\n",
        "parameters = {'C':[0.1, 1, 10], 'gamma':[0.0001, 0.01, 0.1]}\r\n",
        "\r\n",
        "clf = GridSearchCV(svm, parameters)\r\n",
        "clf.fit(x_train, y_train)\r\n",
        "\r\n",
        "# Utility function to report best scores\r\n",
        "def report(results, n_top=3):\r\n",
        "    for i in range(1, n_top + 1):\r\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\r\n",
        "        for candidate in candidates:\r\n",
        "            print(\"Model with rank: {0}\".format(i))\r\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\r\n",
        "                  .format(results['mean_test_score'][candidate],\r\n",
        "                          results['std_test_score'][candidate]))\r\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\r\n",
        "            print(\"\")\r\n",
        "\r\n",
        "report(clf.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKV5X4T2SIM8"
      },
      "source": [
        "print_results(y_train, train_pred, np.unique(y_train).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eMNn3olofIu"
      },
      "source": [
        "print_results(y_valid, valid_pred, np.unique(y_valid).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}