{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/autoencoder_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYgNfSuNfJCu",
        "collapsed": true
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQFvwJpfLSt"
      },
      "source": [
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from google.colab import drive\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "from pathlib import Path\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import pytorch_lightning as pl\r\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXQT6VF9fRAK"
      },
      "source": [
        "drive.mount(\"/content/drive\")\r\n",
        "filepath = \"/content/drive/MyDrive/ml2-eeg-biometrics/eeg_dataset_right_hand_task_subset_9channels.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoKUCENPWgfV"
      },
      "source": [
        "df=pd.read_csv(filepath)\r\n",
        "print(df.shape)\r\n",
        "df.dropna(axis=1,how='all',inplace=True)\r\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stFfQTHJGcI7"
      },
      "source": [
        "# Need to exclude S1 0724 Run2 Trial 10 because of incomplete data.\r\n",
        "df = df[~ ((df['Participant']=='S1') & (df['Date']==20200724) & (df['Run']=='Run2') & (df['Trial']==10))]\r\n",
        "df.reset_index(drop=True, inplace=True) # Reset the index because of the dropped rows.\r\n",
        "\r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FlBa6ByYwbR"
      },
      "source": [
        "# Create a train-validation-test split.\r\n",
        "# For testing we want to take a separate person entirely (let's take S12) - this constitutes 5% of the total dataset.\r\n",
        "# For validation we want to keep it equal across participants and sessions. We'll take 1 run per session for each of the participants. \r\n",
        "# # This is one sixth of the remaining data which equates to 15.8% of the total dataset. Thus, we keep 79.2% for training data.\r\n",
        "\r\n",
        "id_cols = ['Participant','Date', 'Run', 'Task', 'Trial']\r\n",
        "\r\n",
        "# Convert Partipant ID to an integer column 'Target' so that the NN can handle it.\r\n",
        "df['Target'] = df['Participant'].astype('category').cat.codes.values\r\n",
        "\r\n",
        "# test_arr = np.array(df[df['Participant']=='S12'])\r\n",
        "# print(\"test_arr shape:\", test_arr.shape)\r\n",
        "\r\n",
        "# Run5 looks most complete in the data.\r\n",
        "valid_arr = np.array(df[(df['Run']=='Run5') & (df['Participant']!='S12')])\r\n",
        "print(\"valid_arr shape:\", valid_arr.shape)\r\n",
        "\r\n",
        "# # Take the rest for training data\r\n",
        "# train_arr = np.array(df[~ ((df['Run']=='Run5') & (df['Participant']=='S12'))])\r\n",
        "# print(\"train_arr shape:\", train_arr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2BTlVnVCe_4"
      },
      "source": [
        "def reshape_data(arr, n_timepoints=2500):\r\n",
        "  \"\"\" \r\n",
        "  Function to split the ID, target & feature columns and reshape the data into the \r\n",
        "    required format for the AE.\r\n",
        "  Input:\r\n",
        "      arr   np.ndarray containing the 5 ID columns, the channel measurements and integer target (ID)\r\n",
        "  Output: \r\n",
        "    [All np.ndarrays]\r\n",
        "      x_arr   The feature values in shape [n_samples, timepoints, features]\r\n",
        "      y_arr   The (integer) target values in shape [n_samples, 1, 1]\r\n",
        "      id_arr  The categorical identifiers (Date, Run, etc.) in shape [n_samples, 1, 5]\r\n",
        "  \"\"\"\r\n",
        "  x_arr = arr[:,5:14] # Exclude the first 5 ID columns and the 15th column (Target)\r\n",
        "  y_arr = arr[:,14] # Last column - target (participant ID as integer)\r\n",
        "  id_arr = arr[:,:5] # ['Participant','Date', 'Run', 'Task', 'Trial']\r\n",
        "\r\n",
        "  # Find dimensions for new shape.\r\n",
        "  n_rows = len(x_arr)\r\n",
        "  n_samples = int(n_rows/n_timepoints)\r\n",
        "  \r\n",
        "  # Reshape the three arrays to the required shape.\r\n",
        "  x_arr = x_arr.reshape((n_samples, n_timepoints, x_arr.shape[1]))\r\n",
        "  y_arr = y_arr.reshape((n_samples, n_timepoints, 1))\r\n",
        "  id_arr = id_arr.reshape((n_samples, n_timepoints, id_arr.shape[1]))\r\n",
        "\r\n",
        "  # We do not need the target values / ID values to be replicated n_timepoints for each sample.\r\n",
        "  # Reduce these to just 1 value per sample.\r\n",
        "  y_arr_reduced = np.amin(y_arr, axis=1, keepdims=True) # Take the minimum along the second dimension.\r\n",
        "  # Check if this is the same as the max value, if not then something has gone wrong.\r\n",
        "  if not np.all(y_arr_reduced==np.amax(y_arr, axis=1, keepdims=True)):\r\n",
        "    raise ValueError(\"The target value for each sample does not look to be consistent.\")\r\n",
        "\r\n",
        "  id_arr_reduced = np.amin(id_arr, axis=1, keepdims=True) # Take the minimum along the second dimension.\r\n",
        "\r\n",
        "  print(\"x_arr shape: {} \\ny_arr shape: {} \\nid_arr shape: {}\".format(    \\\r\n",
        "                                x_arr.shape, y_arr_reduced.shape, id_arr_reduced.shape))\r\n",
        "\r\n",
        "  return x_arr, y_arr_reduced, id_arr_reduced\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcs8nrnSNZd3"
      },
      "source": [
        "b = np.arange(15*6).reshape(6,15)\r\n",
        "# Ensure we have the same value in the 15th column for all samples from the same target. (n_timepoints=2)\r\n",
        "b[0:3, 14] = 10\r\n",
        "b[3:6, 14] = 20\r\n",
        "print(b)\r\n",
        "reshape_data(b, n_timepoints=3)\r\n",
        "# reshape_data(b, n_timepoints=2) throws an error since the target value is not the same for each sample."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYtwkMKThTP"
      },
      "source": [
        "x_valid, y_valid, id_valid = reshape_data(valid_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JM4ZS-LcIQp"
      },
      "source": [
        "# x_test = torch.tensor(test_df.drop('Target', axis=1).values)\r\n",
        "# y_test = torch.tensor(test_df['Target'].values)\r\n",
        "\r\n",
        "# x_valid = torch.tensor(valid_df.drop('Target', axis=1).values)\r\n",
        "# y_valid = torch.tensor(valid_df['Target'].values)\r\n",
        "\r\n",
        "# x_train = torch.tensor(train_df.drop('Target', axis=1).values)\r\n",
        "# y_train = torch.tensor(train_df['Target'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVewsritg1O9"
      },
      "source": [
        "# df = pd.read_csv('File_explaination.csv')\r\n",
        "# print(df)\r\n",
        "# files = df.get('Filename')\r\n",
        "# print(files[8384])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXIcvQOWxGeW"
      },
      "source": [
        "#Bearing Failure Anomaly Detection\r\n",
        "In this workbook, we use an autoencoder neural network to identify vibrational anomalies from sensor readings in a set of bearings. The goal is to be able to predict future bearing failures before they happen. The vibrational sensor readings are from the NASA Acoustics and Vibration Database. Each data set consists of individual files that are 1-second vibration signal snapshots recorded at 10 minute intervals. Each file contains 20,480 sensor data points that were obtained by reading the bearing sensors at a sampling rate of 20 kHz.\r\n",
        "\r\n",
        "This autoencoder neural network model is created using Long Short-Term Memory (LSTM) recurrent neural network (RNN) cells within the Keras / TensorFlow framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2KjRKO3w2Vk"
      },
      "source": [
        "# import libraries\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.externals import joblib\r\n",
        "import seaborn as sns\r\n",
        "sns.set(color_codes=True)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from numpy.random import seed\r\n",
        "from tensorflow import set_random_seed\r\n",
        "import tensorflow as tf\r\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\r\n",
        "\r\n",
        "\r\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\r\n",
        "from keras.models import Model\r\n",
        "from keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy8BoXaHw8Tr"
      },
      "source": [
        "# set random seed\r\n",
        "seed(10)\r\n",
        "set_random_seed(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaoK5WOUxLWk"
      },
      "source": [
        "#Data loading and pre-processing\r\n",
        "An assumption is that mechanical degradation in the bearings occurs gradually over time; therefore, we use one datapoint every 10 minutes in the analysis. Each 10 minute datapoint is aggregated by using the mean absolute value of the vibration recordings over the 20,480 datapoints in each file. We then merge together everything in a single dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MonWS_15xR2n"
      },
      "source": [
        "data_dir = 'data/bearing_data'\r\n",
        "merged_data = pd.DataFrame()\r\n",
        "\r\n",
        "for filename in os.listdir(data_dir):\r\n",
        "    dataset = pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\r\n",
        "    dataset_mean_abs = np.array(dataset.abs().mean())\r\n",
        "    dataset_mean_abs = pd.DataFrame(dataset_mean_abs.reshape(1,4))\r\n",
        "    dataset_mean_abs.index = [filename]\r\n",
        "    merged_data = merged_data.append(dataset_mean_abs)\r\n",
        "    \r\n",
        "merged_data.columns = ['Bearing 1', 'Bearing 2', 'Bearing 3', 'Bearing 4']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnjIX0PbxVRO"
      },
      "source": [
        "# transform data file index to datetime and sort in chronological order\r\n",
        "merged_data.index = pd.to_datetime(merged_data.index, format='%Y.%m.%d.%H.%M.%S')\r\n",
        "merged_data = merged_data.sort_index()\r\n",
        "merged_data.to_csv('Averaged_BearingTest_Dataset.csv')\r\n",
        "print(\"Dataset shape:\", merged_data.shape)\r\n",
        "merged_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu_TDcD_xhFs"
      },
      "source": [
        "train = merged_data['2004-02-12 10:52:39': '2004-02-15 12:52:39']\r\n",
        "test = merged_data['2004-02-15 12:52:39':]\r\n",
        "print(\"Training dataset shape:\", train.shape)\r\n",
        "print(\"Test dataset shape:\", test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOPVrNrtxjiz"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(train['Bearing 1'], label='Bearing 1', color='blue', animated = True, linewidth=1)\r\n",
        "ax.plot(train['Bearing 2'], label='Bearing 2', color='red', animated = True, linewidth=1)\r\n",
        "ax.plot(train['Bearing 3'], label='Bearing 3', color='green', animated = True, linewidth=1)\r\n",
        "ax.plot(train['Bearing 4'], label='Bearing 4', color='black', animated = True, linewidth=1)\r\n",
        "plt.legend(loc='lower left')\r\n",
        "ax.set_title('Bearing Sensor Training Data', fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naUVTsCdxn0b"
      },
      "source": [
        "# transforming data from the time domain to the frequency domain using fast Fourier transform\r\n",
        "train_fft = np.fft.fft(train)\r\n",
        "test_fft = np.fft.fft(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8inADg4kxoSS"
      },
      "source": [
        "# frequencies of the healthy sensor signal\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(train_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\r\n",
        "ax.plot(train_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\r\n",
        "ax.plot(train_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\r\n",
        "ax.plot(train_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\r\n",
        "plt.legend(loc='lower left')\r\n",
        "ax.set_title('Bearing Sensor Training Frequency Data', fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XXHSgcZxqGQ"
      },
      "source": [
        "# frequencies of the degrading sensor signal\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(test_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\r\n",
        "ax.plot(test_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\r\n",
        "ax.plot(test_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\r\n",
        "ax.plot(test_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\r\n",
        "plt.legend(loc='lower left')\r\n",
        "ax.set_title('Bearing Sensor Test Frequency Data', fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzi-3d-CxyKt"
      },
      "source": [
        "# normalize the data\r\n",
        "scaler = MinMaxScaler()\r\n",
        "X_train = scaler.fit_transform(train)\r\n",
        "X_test = scaler.transform(test)\r\n",
        "scaler_filename = \"scaler_data\"\r\n",
        "joblib.dump(scaler, scaler_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je4mpLs1xysD"
      },
      "source": [
        "# reshape inputs for LSTM [samples, timesteps, features]\r\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\r\n",
        "print(\"Training data shape:\", X_train.shape)\r\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\r\n",
        "print(\"Test data shape:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_mKqxF8ApYW"
      },
      "source": [
        "a = np.arange(40).reshape((10, 4))\r\n",
        "print(a)\r\n",
        "b = a.reshape((5,2,4))\r\n",
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWaAMdQtx5FA"
      },
      "source": [
        "\r\n",
        "# define the autoencoder network model\r\n",
        "def autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(X.shape[1], X.shape[2]))\r\n",
        "    L1 = LSTM(16, activation='relu', return_sequences=True, \r\n",
        "              kernel_regularizer=regularizers.l2(0.00))(inputs)\r\n",
        "    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)\r\n",
        "    L3 = RepeatVector(X.shape[1])(L2)\r\n",
        "    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\r\n",
        "    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\r\n",
        "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \r\n",
        "    model = Model(inputs=inputs, outputs=output)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJjIzolOx8xV"
      },
      "source": [
        "\r\n",
        "# create the autoencoder model\r\n",
        "model = autoencoder_model(X_train)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzzXJTvNyAn0"
      },
      "source": [
        "\r\n",
        "# fit the model to the data\r\n",
        "nb_epochs = 100\r\n",
        "batch_size = 10\r\n",
        "history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\r\n",
        "                    validation_split=0.05).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3LzxyayEBP"
      },
      "source": [
        "# plot the training losses\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\r\n",
        "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\r\n",
        "ax.set_title('Model loss', fontsize=16)\r\n",
        "ax.set_ylabel('Loss (mae)')\r\n",
        "ax.set_xlabel('Epoch')\r\n",
        "ax.legend(loc='upper right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfIG-Na0Rim"
      },
      "source": [
        "\r\n",
        "#Distribution of Loss Function\r\n",
        "By plotting the distribution of the calculated loss in the training set, one can use this to identify a suitable threshold value for identifying an anomaly. In doing this, one can make sure that this threshold is set above the “noise level” and that any flagged anomalies should be statistically significant above the background noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERj4N100Maz"
      },
      "source": [
        "# plot the loss distribution of the training set\r\n",
        "X_pred = model.predict(X_train)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\r\n",
        "X_pred = pd.DataFrame(X_pred, columns=train.columns)\r\n",
        "X_pred.index = train.index\r\n",
        "\r\n",
        "scored = pd.DataFrame(index=train.index)\r\n",
        "Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])\r\n",
        "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis = 1)\r\n",
        "plt.figure(figsize=(16,9), dpi=80)\r\n",
        "plt.title('Loss Distribution', fontsize=16)\r\n",
        "sns.distplot(scored['Loss_mae'], bins = 20, kde= True, color = 'blue');\r\n",
        "plt.xlim([0.0,.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVboI1U00c9h"
      },
      "source": [
        "# calculate the loss on the test set\r\n",
        "X_pred = model.predict(X_test)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\r\n",
        "X_pred = pd.DataFrame(X_pred, columns=test.columns)\r\n",
        "X_pred.index = test.index\r\n",
        "\r\n",
        "scored = pd.DataFrame(index=test.index)\r\n",
        "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\r\n",
        "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\r\n",
        "scored['Threshold'] = 0.275\r\n",
        "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\r\n",
        "scored.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1sLxu6V0frb"
      },
      "source": [
        "# calculate the same metrics for the training set \r\n",
        "# and merge all data in a single dataframe for plotting\r\n",
        "X_pred_train = model.predict(X_train)\r\n",
        "X_pred_train = X_pred_train.reshape(X_pred_train.shape[0], X_pred_train.shape[2])\r\n",
        "X_pred_train = pd.DataFrame(X_pred_train, columns=train.columns)\r\n",
        "X_pred_train.index = train.index\r\n",
        "\r\n",
        "scored_train = pd.DataFrame(index=train.index)\r\n",
        "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-Xtrain), axis = 1)\r\n",
        "scored_train['Threshold'] = 0.275\r\n",
        "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\r\n",
        "scored = pd.concat([scored_train, scored])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WwrSeZ30i7v"
      },
      "source": [
        "# plot bearing failure time plot\r\n",
        "scored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVIfqljx0lwt"
      },
      "source": [
        "\r\n",
        "# save all model information, including weights, in h5 format\r\n",
        "model.save(\"Cloud_model.h5\")\r\n",
        "print(\"Model saved\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}