{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/fc_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTCrOSwlbGd"
      },
      "source": [
        "## Load libraries & initialise environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQFvwJpfLSt"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# import libraries\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from copy import deepcopy\r\n",
        "from sklearn.externals import joblib\r\n",
        "import seaborn as sns\r\n",
        "sns.set(color_codes=True)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from numpy.random import seed\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, Reshape, \\\r\n",
        "          RepeatVector, MaxPooling1D, Conv1D, Flatten, Conv1DTranspose, UpSampling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras import regularizers\r\n",
        "\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXQT6VF9fRAK"
      },
      "source": [
        "drive.mount(\"/content/drive\")\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6YB2XBaXlTy"
      },
      "source": [
        "# set random seed\r\n",
        "seed(10)\r\n",
        "tf.compat.v1.set_random_seed(10)\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeTPxR0aYNxV"
      },
      "source": [
        "## Load & Scale Data\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqlAto997Wp"
      },
      "source": [
        "x_train_unscaled = np.load(dirpath + 'x_train.npy')\r\n",
        "# y_train = np.load(dirpath + 'y_train.npy')\r\n",
        "\r\n",
        "x_test_unscaled = np.load(dirpath + 'x_test.npy', allow_pickle=True)\r\n",
        "\r\n",
        "print(x_train_unscaled.shape, x_test_unscaled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je6ycNzG3EMw"
      },
      "source": [
        "cols = ['Statistic','F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "def data_summary(dataset):\r\n",
        "  \"\"\" \r\n",
        "  input:\r\n",
        "    dataset     the three dimensional input (n_samples, n_timepoints, n_features) \r\n",
        "\r\n",
        "    Prints histograms for the 9 features individually\r\n",
        "  returns: \r\n",
        "    summ_df     pd.DataFrame containing summary statistics for the 9 features.\r\n",
        "  \"\"\"\r\n",
        "  data = dataset.reshape((dataset.shape[0] * dataset.shape[1], dataset.shape[2]))\r\n",
        "  \r\n",
        "  min = data.min(axis=0).reshape(1, data.shape[1])\r\n",
        "  max = data.max(axis=0).reshape(1, data.shape[1])\r\n",
        "  mean = data.mean(axis=0).reshape(1, data.shape[1])\r\n",
        "  var = data.var(axis=0).reshape(1, data.shape[1])\r\n",
        "  q01 = np.quantile(data, 0.01, axis=0).reshape(1, data.shape[1])\r\n",
        "  q99 = np.quantile(data, 0.99, axis=0).reshape(1, data.shape[1])\r\n",
        "\r\n",
        "  names=np.array([['min','max','mean','var','1st percentile', '99th percentile']]).reshape(6,1)\r\n",
        "\r\n",
        "  summ = np.concatenate((names, np.concatenate((min,max,mean,var,q01,q99),axis=0)), axis=1)\r\n",
        "  summ_df = pd.DataFrame(summ, columns=cols)\r\n",
        "\r\n",
        "  # Plot histograms per channel.\r\n",
        "  fig, axes = plt.subplots(3,3, figsize = (9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "  for i in range(9):\r\n",
        "    axes[i].hist(data[:,i], range= (q01[0,i], q99[0,i]),   density=True)\r\n",
        "    axes[i].title.set_text(cols[i+1])\r\n",
        "  fig.suptitle(\"Distribution for each channel (between 1st & 99th percentile)\", size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])\r\n",
        "\r\n",
        "  return summ_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPhAKBtY04dN"
      },
      "source": [
        "## Plot distributions of each channel.\r\n",
        "# data_summary(x_train_unscaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IW7QFavd7M"
      },
      "source": [
        "#### Smooth out extreme points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbOssMQHviNv"
      },
      "source": [
        "# Reshape to 2 dimensional array\r\n",
        "x_train = deepcopy(x_train_unscaled)\r\n",
        "x_test = deepcopy(x_test_unscaled)\r\n",
        "\r\n",
        "x_train = x_train.reshape((x_train.shape[0]*2500, x_train.shape[2]))\r\n",
        "x_test = x_test.reshape((x_test.shape[0]*2500, x_test.shape[2]))\r\n",
        "\r\n",
        "# Find the 1st & 99th percentiles for each column of the training data.\r\n",
        "q01  = np.quantile(x_train, 0.01, axis=0)\r\n",
        "q99  = np.quantile(x_train, 0.99, axis=0)\r\n",
        "\r\n",
        "# Loop through the columns and apply the cutoff\r\n",
        "for i in range(x_train.shape[1]):\r\n",
        "  x_train[x_train[:,i] < q01[i], i] = q01[i] # If the value is below the 1st percentile, replace with the 1st percentile.\r\n",
        "  x_train[x_train[:,i] > q99[i], i] = q99[i] # If the value is above the 99th percentile, replace with the 99th percentile.\r\n",
        "  # Do the same with the test data, using the cutoffs calculated from the training data.\r\n",
        "  x_test[x_test[:,i] < q01[i], i] = q01[i] \r\n",
        "  x_test[x_test[:,i] > q99[i], i] = q99[i] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzi-3d-CxyKt"
      },
      "source": [
        "# normalize the data\r\n",
        "scaler = MinMaxScaler()\r\n",
        "\r\n",
        "x_train = scaler.fit_transform(x_train)\r\n",
        "x_test = scaler.transform(x_test)\r\n",
        "scaler_filename = \"/content/drive/MyDrive/ml2-eeg-biometrics/scaler_data\"\r\n",
        "joblib.dump(scaler, scaler_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je4mpLs1xysD"
      },
      "source": [
        "# Take a subset of the data - only one feature, every 5 timepoints, to test different AE configurations.\r\n",
        "x_train_1f = x_train[::5,0]\r\n",
        "# x_train_1f = x_train_1f.reshape((int(len(x_train_1f)/500), 500)) # Reshape so that the timepoints represent columns. (for Fully Connected autoencoder only)\r\n",
        "\r\n",
        "# Convolutional layer requires the shape (n_samples, n_timepoints, n_features)\r\n",
        "x_train_1f = x_train_1f.reshape((int(len(x_train_1f)/500), 500, 1)) # We're only taking one channel for now, so last dimension is 1.\r\n",
        "print(x_train_1f.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HBB2Vpa0Zg7"
      },
      "source": [
        "# data_summary(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7XEKeEo8a5"
      },
      "source": [
        "## Visualise the Raw & Scaled Signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DEghaTsYrFO"
      },
      "source": [
        "labels=[(('F3',0), ('F4',1)), (('FC3',2), ('FC4',3)), (('C3',4), ('Cz',5), ('C4',6)), (('CP3',7), ('CP4',8))]\r\n",
        "colours= ['darkslateblue', 'orange','lightskyblue','brown','darkgreen','darkgrey','bisque','violet','palegreen']\r\n",
        "\r\n",
        "def plot_signals(sample, title=None):\r\n",
        "  fig, axes = plt.subplots(2,2, figsize = (6,6))\r\n",
        "  axes=axes.ravel()\r\n",
        "  plt.suptitle(\"Signals\" if title is None else title, size=16)\r\n",
        "  count=0\r\n",
        "  for label_group in labels:\r\n",
        "    for label, ind in label_group:\r\n",
        "      axes[count].plot(sample[:,ind], label=label,color=colours[ind], alpha=0.8)\r\n",
        "      axes[count].legend()\r\n",
        "    count+=1\r\n",
        "\r\n",
        "# plot_signals(x_train[101], title=\"Scaled Signals - x_train[0]\")\r\n",
        "# plot_signals(x_train_unscaled[101], title=\"Unscaled Signals - x_train[0]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YonvAGL9lmn6"
      },
      "source": [
        "\r\n",
        "#### LSTM Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWaAMdQtx5FA"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "def autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(X.shape[1], X.shape[2]))\r\n",
        "    L1 = LSTM(18, activation='tanh', return_sequences=True, \r\n",
        "              kernel_regularizer=regularizers.l2(0.00))(inputs)\r\n",
        "    L2 = LSTM(450, activation='tanh', return_sequences=False)(L1)\r\n",
        "    L3 = RepeatVector(X.shape[1])(L2)\r\n",
        "    L4 = LSTM(450, activation='tanh', return_sequences=True)(L3)\r\n",
        "    L5 = LSTM(18, activation='tanh', return_sequences=True)(L4)\r\n",
        "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \r\n",
        "    model = Model(inputs=inputs, outputs=output)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTUoSyKlypi7"
      },
      "source": [
        "#### Fully Connected Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okqWORve_W0b"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "def fc_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(500,))\r\n",
        "    e1 = Dense(225, activation='relu')(inputs)\r\n",
        "    e2 = Dense(150, activation='relu')(e1)\r\n",
        "    e3 = Dense(70, activation='relu')(e2)  \r\n",
        "    encoded = Dense(70, activation='relu')(e3)\r\n",
        "    d1 = Dense(70, activation='relu')(encoded)  \r\n",
        "    d2 = Dense(150, activation='relu')(d1)\r\n",
        "    d3 = Dense(225, activation='relu')(d2)\r\n",
        "    decoded = Dense(X.shape[1], activation='sigmoid')(d3)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=decoded)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJjIzolOx8xV"
      },
      "source": [
        "# create the autoencoder model\r\n",
        "model = fc_autoencoder_model(x_train_1f)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUsOKCEyxv1"
      },
      "source": [
        "#### Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtK53tcGzfL3"
      },
      "source": [
        "### Testing only, not a working model.\r\n",
        "inputs = Input(shape=(500,1))\r\n",
        "t1 =  Conv1D(10, 9, padding='same')(inputs)\r\n",
        "t2 = MaxPooling1D(2)(t1)\r\n",
        "# t3 = Flatten()(t2)\r\n",
        "\r\n",
        "t4 =  Conv1DTranspose(1, 9, padding='same')(t2)\r\n",
        "\r\n",
        "t5 = Conv1D(4,5, padding='same')(t2)\r\n",
        "\r\n",
        "t6 = MaxPooling1D(3)(t4)\r\n",
        "\r\n",
        "test = Model(inputs=inputs, outputs=t2)\r\n",
        "test.output_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4dPRBsVy2Ot"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "# Testing around with adding or removing certain layers, no huge difference in performance vs. just using a simpler architecture outlined a few cells down.\r\n",
        "def conv_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(500,1))\r\n",
        "    e1 = Conv1D(4, 9, activation='relu', padding='same')(inputs)\r\n",
        "    e2 = MaxPooling1D(2)(e1)\r\n",
        "    e3 = Conv1D(2,5, activation = 'relu', padding='same')(e2)\r\n",
        "    e4 = MaxPooling1D(5)(e3)\r\n",
        "    # e5 = Flatten()(e4)\r\n",
        "    # encoded = Dense(100, activation='relu')(e5)\r\n",
        "    # d1 = Dense(100, activation='relu')(encoded)\r\n",
        "    # d2 = Reshape((50,2))(d1)\r\n",
        "    d3 = UpSampling1D(5)(e4)\r\n",
        "    d4 = Conv1DTranspose(4, 5, activation='relu', padding='same')(d3)\r\n",
        "    d5 = UpSampling1D(2)(d4)\r\n",
        "    d6 = Conv1DTranspose(1, 9, activation='relu', padding='same')(d5)\r\n",
        "    # decoded = Dense(500, activation='sigmoid')(d6)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=d6)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge5xe61zy4AL"
      },
      "source": [
        "# create the autoencoder model\r\n",
        "model = conv_autoencoder_model(x_train_1f)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgn_4J2ARQCW"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "def simple_conv_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(500,1))\r\n",
        "    e1 = Conv1D(4, 9, activation='relu', padding='same')(inputs)\r\n",
        "    e2 = MaxPooling1D(10)(e1)\r\n",
        "    # e3 = Conv1D(2,5, activation = 'relu', padding='same')(e2)\r\n",
        "    # e4 = MaxPooling1D(5)(e3)\r\n",
        "    # e5 = Flatten()(e4)\r\n",
        "    # encoded = Dense(100, activation='relu')(e5)\r\n",
        "    # d1 = Dense(100, activation='relu')(encoded)\r\n",
        "    # d2 = Reshape((50,2))(d1)\r\n",
        "    # d3 = UpSampling1D(5)(d2)\r\n",
        "    # d4 = Conv1DTranspose(4, 5, activation='relu', padding='same')(d3)\r\n",
        "    d5 = UpSampling1D(10)(e2)\r\n",
        "    d6 = Conv1DTranspose(1, 9, activation='sigmoid', padding='same')(d5)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=d6)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJlY5HJdR6hf"
      },
      "source": [
        "# create the simple autoencoder model\r\n",
        "model = simple_conv_autoencoder_model(x_train_1f)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzzXJTvNyAn0"
      },
      "source": [
        "# fit the model to the data\r\n",
        "nb_epochs = 100\r\n",
        "batch_size = 10\r\n",
        "history = model.fit(x_train_1f, x_train_1f, epochs=nb_epochs,\r\n",
        "                    batch_size=batch_size,\r\n",
        "                    # validation_split=0.05\r\n",
        "                    ).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbI4PWeyKxQT"
      },
      "source": [
        "# Plot samples.\r\n",
        "x_pred = model.predict(x_train_1f)\r\n",
        "\r\n",
        "# Plot actuals vs. prediction for one column across all rows\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.scatter(x_train_1f[:,0], x_pred[:,0],alpha=0.2)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Plot actuals vs. predictions for one row across all columns\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_train_1f[0,:], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[0,:], label= 'predicted',alpha=0.7)\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "# Plot same as above for a different row\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_train_1f[1039,:], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[1039,:], label= 'predicted',alpha=0.7)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3LzxyayEBP"
      },
      "source": [
        "# plot the training losses\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\r\n",
        "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\r\n",
        "ax.set_title('Model loss', fontsize=16)\r\n",
        "ax.set_ylabel('Loss (mae)')\r\n",
        "ax.set_xlabel('Epoch')\r\n",
        "ax.legend(loc='upper right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfIG-Na0Rim"
      },
      "source": [
        "\r\n",
        "## Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AvXQtdVLPT"
      },
      "source": [
        "# Get the predicted values for the training set.\r\n",
        "X_pred_3D = model.predict(x_train)\r\n",
        "X_pred = X_pred_3D.reshape(X_pred_3D.shape[0]*X_pred_3D.shape[1], X_pred_3D.shape[2])\r\n",
        "# X_pred = pd.DataFrame(X_pred, columns=train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERj4N100Maz"
      },
      "source": [
        "# Plot the distribution of the loss\r\n",
        "x_train_reshaped = x_train.reshape(x_train.shape[0]*x_train.shape[1], x_train.shape[2])\r\n",
        "loss_mae = np.mean(np.abs(X_pred-x_train_reshaped), axis = 1)\r\n",
        "plt.figure(figsize=(16,9), dpi=80)\r\n",
        "plt.title('Loss Distribution', fontsize=16)\r\n",
        "sns.distplot(loss_mae, bins = 20, kde= True, color = 'blue');\r\n",
        "plt.xlim([0.0,.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er4v0JqLR-0M"
      },
      "source": [
        "cols = ['F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "\r\n",
        "def evaluate_prediction(actuals, pred, ind, rescale=False):\r\n",
        "  \"\"\" Function to plot predictions vs. the actuals for one sample.\r\n",
        "  input:\r\n",
        "    actuals   3D array (n_samplesx2500x9) - Original scaled signals.\r\n",
        "    pred      the predicted values corresponding to the actuals.\r\n",
        "    ind     The row number of the sample (2500x9) that you want to compare.\r\n",
        "    rescale   If set to true then the data is first converted back to the original scale for comparison.\r\n",
        "  \r\n",
        "  returns:\r\n",
        "    nothing\r\n",
        "\r\n",
        "  prints plots.\r\n",
        "  \"\"\"\r\n",
        "  if rescale:\r\n",
        "    sample_actual = scaler.inverse_transform(actuals[ind]) # Rescale to the original scale.\r\n",
        "    sample_pred = scaler.inverse_transform(pred[ind]) \r\n",
        "  else:\r\n",
        "    sample_actual = actuals[ind]    # Get the relevant sample.\r\n",
        "    sample_pred = pred[ind]\r\n",
        "\r\n",
        "  mae_by_channel = np.mean(np.abs(sample_pred - sample_actual), axis=0) # Get the Mean Absolute Error for each channel for this sample\r\n",
        "  sample_mae = np.mean(mae_by_channel) # Get the total MAE for the sample by taking the average across the 9 channels\r\n",
        "  print(\"Sample\", ind, \"\\n   Total Mean Absolute Error:\", round(sample_mae, 8))\r\n",
        "  print(\"Mean Absolute Error by Channel:\")\r\n",
        "  for col, error in zip(cols, mae_by_channel):\r\n",
        "    print(col, \": \", round(error,8)) \r\n",
        "\r\n",
        "  fig, axes = plt.subplots(3,3, figsize=(9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "\r\n",
        "  for i in range(9):\r\n",
        "    axes[i].plot(sample_actual[:,i], label= \"Actual\")\r\n",
        "    axes[i].plot(sample_pred[:,i], label=\"Predicted\")\r\n",
        "    axes[i].title.set_text(cols[i] + str(round(mae_by_channel[i], 3)))\r\n",
        "  \r\n",
        "  plt.legend()\r\n",
        "  fig.suptitle(\"Predictions vs. Actuals - Sample \" + str(ind),size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])\r\n",
        "\r\n",
        "evaluate_prediction(x_train, X_pred_3D, ind=1, rescale=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HiUfx3u3N1N"
      },
      "source": [
        "X_pred_3D[3000][1000:1100, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uWEwvqvU36m"
      },
      "source": [
        "np.mean(np.abs(X_pred_3D[0] - x_train[0]), axis=1)[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2FB1JADjIE"
      },
      "source": [
        "# plot the loss distribution of the test set\r\n",
        "X_pred = model.predict(x_test)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0]*X_pred.shape[1], X_pred.shape[2])\r\n",
        "\r\n",
        "x_test_reshaped = x_test.reshape(x_test.shape[0]*x_test.shape[1], x_test.shape[2])\r\n",
        "fig, axes = plt.subplots(9,1, figsize=(18,9))\r\n",
        "# Plot the loss distribution for each channel individually\r\n",
        "for i in range(x_test_reshaped.shape[1]):\r\n",
        "  loss_mae = np.abs(X_pred[:,i]-x_test_reshaped[:,i])\r\n",
        "  sns.distplot(loss_mae, bins = 100, kde= True, color = 'blue', ax=axes[i]);\r\n",
        "  axes[i].axis(xmin=0.0,xmax=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVIfqljx0lwt"
      },
      "source": [
        "# save all model information, including weights, in h5 format\r\n",
        "model.save(\"/content/drive/MyDrive/ml2-eeg-biometrics/model1_rw.h5\")\r\n",
        "print(\"Model saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-_9EyL8dscb"
      },
      "source": [
        "# Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnTNfQ69drU_"
      },
      "source": [
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from numpy import dstack\r\n",
        "from pandas import read_csv\r\n",
        "from matplotlib import pyplot\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lob7iKO-d1NH"
      },
      "source": [
        "# cnn model\r\n",
        "\r\n",
        "# fit and evaluate a model\r\n",
        "def evaluate_model(trainX, trainy, testX, testy, n_filters):\r\n",
        "  verbose, epochs, batch_size = 0, 10, 32\r\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\r\n",
        "  model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu'))\r\n",
        "  model.add(Dropout(0.5))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Flatten())\r\n",
        "  model.add(Dense(100, activation='relu'))\r\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  # fit network\r\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\r\n",
        "  # evaluate model\r\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\r\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK4gsDmgd5le"
      },
      "source": [
        "# summarize scores\r\n",
        "def summarize_results(scores, params):\r\n",
        "  print(scores, params)\r\n",
        "  # summarize mean and standard deviation\r\n",
        "  for i in range(len(scores)):\r\n",
        "    m, s = mean(scores[i]), std(scores[i])\r\n",
        "    print('Param=%d: %.3f%% (+/-%.3f)' % (params[i], m, s))\r\n",
        "  # boxplot of scores\r\n",
        "  pyplot.boxplot(scores, labels=params)\r\n",
        "  pyplot.savefig('exp_cnn_filters.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rv7wgcnd8jn"
      },
      "source": [
        "# run an experiment\r\n",
        "def run_experiment(params, repeats=10):\r\n",
        "  # test each parameter\r\n",
        "  all_scores = list()\r\n",
        "  for p in params:\r\n",
        "    # repeat experiment\r\n",
        "    scores = list()\r\n",
        "    for r in range(repeats):\r\n",
        "      score = evaluate_model(x_train, y_train, x_test, y_test, p)\r\n",
        "      score = score * 100.0\r\n",
        "      print('>p=%d #%d: %.3f' % (p, r+1, score))\r\n",
        "      scores.append(score)\r\n",
        "    all_scores.append(scores)\r\n",
        "  # summarize results\r\n",
        "  summarize_results(all_scores, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqlXCQ8DeBnQ"
      },
      "source": [
        "# run the experiment\r\n",
        "n_params = [8, 16, 32, 64, 128, 256]\r\n",
        "run_experiment(n_params)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}