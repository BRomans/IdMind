{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRomans/IdMind/blob/main/fc_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTCrOSwlbGd"
      },
      "source": [
        "## Load libraries & initialise environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sQFvwJpfLSt"
      },
      "source": [
        "# import libraries\r\n",
        "from google.colab import drive\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from copy import deepcopy\r\n",
        "from sklearn.externals import joblib\r\n",
        "import seaborn as sns\r\n",
        "sns.set(color_codes=True)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from numpy.random import seed\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, Reshape, \\\r\n",
        "          RepeatVector, MaxPooling1D, Conv1D, Flatten, Conv1DTranspose, UpSampling1D\r\n",
        "from keras.models import Model\r\n",
        "from keras import regularizers\r\n",
        "\r\n",
        "from tensorflow.python.client import device_lib\r\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXQT6VF9fRAK"
      },
      "source": [
        "drive.mount(\"/content/drive\")\r\n",
        "dirpath = \"/content/drive/MyDrive/ml2-eeg-biometrics/train-test-data/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6YB2XBaXlTy"
      },
      "source": [
        "# set random seed\r\n",
        "seed(10)\r\n",
        "tf.compat.v1.set_random_seed(10)\r\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeTPxR0aYNxV"
      },
      "source": [
        "## Load & Process Data\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BInAGYqORGnb"
      },
      "source": [
        "##### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAqlAto997Wp"
      },
      "source": [
        "x_train_unscaled = np.load(dirpath + 'x_train.npy')\r\n",
        "# y_train = np.load(dirpath + 'y_train.npy')\r\n",
        "# id_train = np.load(dirpath + 'id_train.npy', allow_pickle=True)\r\n",
        "\r\n",
        "x_test_unscaled = np.load(dirpath + 'x_test.npy', allow_pickle=True)\r\n",
        "\r\n",
        "print(x_train_unscaled.shape, x_test_unscaled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EporEztRJ5L"
      },
      "source": [
        "##### Plot distributions of unscaled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je6ycNzG3EMw"
      },
      "source": [
        "cols = ['Statistic','F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "def data_summary(dataset):\r\n",
        "  \"\"\" \r\n",
        "  input:\r\n",
        "    dataset     the three dimensional input (n_samples, n_timepoints, n_features) \r\n",
        "\r\n",
        "    Prints histograms for the 9 features individually\r\n",
        "  returns: \r\n",
        "    summ_df     pd.DataFrame containing summary statistics for the 9 features.\r\n",
        "  \"\"\"\r\n",
        "  data = dataset.reshape((dataset.shape[0] * dataset.shape[1], dataset.shape[2])) # Reshape to 2D (n_samples*n_timepoints, n_features)\r\n",
        "  \r\n",
        "  # Calculate the summary statistics.\r\n",
        "  min   = data.min(axis=0).reshape(1, data.shape[1])                  # Calculate the minimum over the rows for each column.\r\n",
        "  max   = data.max(axis=0).reshape(1, data.shape[1])                  # Then reshape the result to one row and n_cols=n_features, to make it easier to combine later.\r\n",
        "  mean  = data.mean(axis=0).reshape(1, data.shape[1])\r\n",
        "  var   = data.var(axis=0).reshape(1, data.shape[1])\r\n",
        "  q01   = np.quantile(data, 0.01, axis=0).reshape(1, data.shape[1])\r\n",
        "  q99   = np.quantile(data, 0.99, axis=0).reshape(1, data.shape[1])\r\n",
        "\r\n",
        "  names=np.array([['min','max','mean','var','1st percentile', '99th percentile']]).reshape(6,1) # Create a column of names for the summary stats.\r\n",
        "  stats = np.concatenate((min,max,mean,var,q01,q99), axis=0)          # Combine the summary stats in one array\r\n",
        "\r\n",
        "  summ = np.concatenate((names, np.round(stats, 4)), axis=1)          # Combine the summary stats with their names.\r\n",
        "  summ_df = pd.DataFrame(summ, columns=cols)                          # Create a dataframe and supply the channel names as columns.\r\n",
        "\r\n",
        "  # Plot histograms per channel.\r\n",
        "  fig, axes = plt.subplots(3,3, figsize = (9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "  for i in range(9): # Loop through the channels.\r\n",
        "    axes[i].hist(data[:,i], range= (q01[0,i], q99[0,i]),   density=True)    # Add histogram subplot for the values of that channel.\r\n",
        "    axes[i].title.set_text(cols[i+1])                                       # Add a title with the channel name.\r\n",
        "  fig.suptitle(\"Distribution for each channel (between 1st & 99th percentile)\", size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])                                 # Cut the plot space to make space for the global title.\r\n",
        "\r\n",
        "  return summ_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPhAKBtY04dN"
      },
      "source": [
        "# Plot distributions of each channel.\r\n",
        "unscaled_summary = data_summary(x_train_unscaled)\r\n",
        "unscaled_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obsst4fTzSK4"
      },
      "source": [
        "### Band-pass filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6M8UO7FRueQ"
      },
      "source": [
        "##### Create the filters and apply across the whole data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMKQR0SzQ47"
      },
      "source": [
        "from scipy import signal\r\n",
        "from copy import deepcopy\r\n",
        "\r\n",
        "low_cut = 0.5\r\n",
        "high_cut = 33.0\r\n",
        "\r\n",
        "bp = signal.butter(10, (low_cut,high_cut), 'bp', fs=500, output='sos') # Create the filter. fs is the sampling rate.\r\n",
        "hp = signal.butter(10, low_cut, 'hp', fs=500, output='sos') # Create the a high pass filter for testing.\r\n",
        "lp = signal.butter(10, high_cut, 'lp', fs=500, output='sos') # Create the a high pass filter for testing.\r\n",
        "\r\n",
        "# Create copies of the data\r\n",
        "# x_train_filtered = deepcopy(x_train_unscaled)         # After running once in the session, I comment these out because otherwise if you re-run the cell it eats RAM.\r\n",
        "# x_test_filtered = deepcopy(x_test_unscaled)\r\n",
        "\r\n",
        "print(x_train_filtered.shape)\r\n",
        "\r\n",
        "# Use one sample and one channel for the purpose of visualising the effects of different filters.\r\n",
        "bp_signal = signal.sosfilt(bp, x_train_filtered[1039,:,0])\r\n",
        "hp_signal = signal.sosfilt(hp, x_train_filtered[0,:,0])\r\n",
        "lp_signal = signal.sosfilt(lp, x_train_filtered[0,:,0])\r\n",
        "\r\n",
        "x_train_filtered = signal.sosfilt(bp, x_train_filtered, axis=1)\r\n",
        "\r\n",
        "bp_signal.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn-9vyMQRT4E"
      },
      "source": [
        "##### Test if application of band-pass filter across whole array is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZdSrbZJvLG"
      },
      "source": [
        "# Test if application to the whole array is correct.\r\n",
        "bp_test = signal.sosfilt(bp, x_train_unscaled[2000,:,3])  # Take one sample of one channel in the middle of the data\r\n",
        "\r\n",
        "plt.plot(bp_test)                         # Plot a bandpass filter applied to the single sample.\r\n",
        "plt.title(\"BP - Test Sample\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.plot(x_train_filtered[2000,:,3])         # Plot the same sample from the wholly filtered array.\r\n",
        "plt.title(\"BP applied to whole array\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.plot(x_train_unscaled[2000,:,3])      # Plot the original sample for comparison.\r\n",
        "plt.title(\"Original Sample\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfHwqolgR1Mj"
      },
      "source": [
        "##### Visualise effect of different filters on individual samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHRXSiFw3h3m"
      },
      "source": [
        " # Visualise the effects of different filters and cut-off frequencies on single samples.\r\n",
        "plt.figure(figsize=(18,10))\r\n",
        "plt.plot(x_train_unscaled[1039,:,0], label = 'Original', alpha=0.3)\r\n",
        "plt.plot(bp_signal, label = 'Band-pass', alpha=0.7)\r\n",
        "# plt.plot(hp_signal, label = 'High-pass', alpha=0.7)\r\n",
        "# plt.plot(lp_signal, label = 'Low-pass', alpha=0.6)\r\n",
        "\r\n",
        "plt.title(\"Effect of different filters (high-pass @ {}Hz, low-pass @ {}Hz) Sample 0\".format(low_cut,high_cut))\r\n",
        "# plt.title(\"Effect of different filters (low-pass @ {}Hz) Sample 0\".format(high_cut))\r\n",
        "# plt.title(\"Effect of different filters (high-pass @ {}Hz) Sample 0\".format(low_cut))\r\n",
        "\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQitBfHU_z4l"
      },
      "source": [
        "##### Test effect of filter on outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMvuUP0YkZc3"
      },
      "source": [
        "max   = x_train_unscaled.max(axis=1)     \r\n",
        "max.shape\r\n",
        "\r\n",
        "# plt.hist(max[:,1], bins=100)\r\n",
        "inds = np.where(max[:,1] > 200)\r\n",
        "\r\n",
        "num=10\r\n",
        "fig, axes = plt.subplots(num, 2, figsize = (12,12))\r\n",
        "\r\n",
        "for i in range(num):\r\n",
        "  axes[i, 0].plot(x_train_unscaled[inds[0][i],:,1])\r\n",
        "  axes[i, 1].plot(bp_signal_arr[inds[0][i],:,1])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOK0DnGQR95A"
      },
      "source": [
        "##### Plot distributions of the total dataset after the band-pass filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG_WtSqx15NJ"
      },
      "source": [
        "data_summary(x_train_filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IW7QFavd7M"
      },
      "source": [
        "### Smooth out extreme points beyond the 1st-99th percentile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbOssMQHviNv"
      },
      "source": [
        "# Reshape to 2 dimensional array\r\n",
        "x_train = deepcopy(x_train_unscaled)\r\n",
        "x_test = deepcopy(x_test_unscaled)\r\n",
        "\r\n",
        "x_train = x_train.reshape((x_train.shape[0]*2500, x_train.shape[2]))\r\n",
        "x_test = x_test.reshape((x_test.shape[0]*2500, x_test.shape[2]))\r\n",
        "\r\n",
        "# Find the 1st & 99th percentiles for each column of the training data.\r\n",
        "q01  = np.quantile(x_train, 0.01, axis=0)\r\n",
        "q99  = np.quantile(x_train, 0.99, axis=0)\r\n",
        "\r\n",
        "# Loop through the columns and apply the cutoff\r\n",
        "for i in range(x_train.shape[1]):\r\n",
        "  x_train[x_train[:,i] < q01[i], i] = q01[i] # If the value is below the 1st percentile, replace with the 1st percentile.\r\n",
        "  x_train[x_train[:,i] > q99[i], i] = q99[i] # If the value is above the 99th percentile, replace with the 99th percentile.\r\n",
        "  # Do the same with the test data, using the cutoffs calculated from the training data.\r\n",
        "  x_test[x_test[:,i] < q01[i], i] = q01[i] \r\n",
        "  x_test[x_test[:,i] > q99[i], i] = q99[i] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpn8rNubT0LF"
      },
      "source": [
        "### Scale the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzi-3d-CxyKt"
      },
      "source": [
        "# normalize the data\r\n",
        "scaler = MinMaxScaler()\r\n",
        "\r\n",
        "x_train = scaler.fit_transform(x_train)\r\n",
        "x_test = scaler.transform(x_test)\r\n",
        "scaler_filename = \"/content/drive/MyDrive/ml2-eeg-biometrics/scaler_data\"\r\n",
        "joblib.dump(scaler, scaler_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fvTtfV8evvj"
      },
      "source": [
        "# Scaling applied to the filtered signal.\r\n",
        "scaler_filtered = MinMaxScaler()\r\n",
        "\r\n",
        "n_samples, n_timepoints, n_features = x_train_filtered.shape\r\n",
        "n_samples_test =  x_test_filtered.shape[0]\r\n",
        "\r\n",
        "# Reshape to 2D for the scaler.\r\n",
        "x_train_filtered = x_train_filtered.reshape((n_samples*n_timepoints, n_features))\r\n",
        "# x_test_filtered = x_test_filtered.reshape((n_samples_test*n_timepoints, n_features))\r\n",
        "\r\n",
        "# Apply the scaling.\r\n",
        "x_train_filtered = scaler_filtered.fit_transform(x_train_filtered)\r\n",
        "# x_test_filtered = scaler_filtered.transform(x_test_filtered)\r\n",
        "\r\n",
        "# Re-shape back to 3D for the convolutional autoencoder.\r\n",
        "x_train_filtered = x_train_filtered.reshape((n_samples, n_timepoints, n_features))    \r\n",
        "# x_test_filtered = x_test_filtered.reshape((n_samples_test, n_timepoints, n_features))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Xfm82dUNWF"
      },
      "source": [
        "# data_summary(x_train)     # Plot the distribution of the scaled data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7XEKeEo8a5"
      },
      "source": [
        "##### Visualise the Raw & Scaled Signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DEghaTsYrFO"
      },
      "source": [
        "labels=[(('F3',0), ('F4',1)), (('FC3',2), ('FC4',3)), (('C3',4), ('Cz',5), ('C4',6)), (('CP3',7), ('CP4',8))]\r\n",
        "colours= ['darkslateblue', 'orange','lightskyblue','brown','darkgreen','darkgrey','bisque','violet','palegreen']\r\n",
        "\r\n",
        "def plot_signals(sample, title=None):\r\n",
        "  fig, axes = plt.subplots(2,2, figsize = (6,6))\r\n",
        "  axes=axes.ravel()\r\n",
        "  plt.suptitle(\"Signals\" if title is None else title, size=16)\r\n",
        "  count=0\r\n",
        "  for label_group in labels:\r\n",
        "    for label, ind in label_group:\r\n",
        "      axes[count].plot(sample[:,ind], label=label,color=colours[ind], alpha=0.8)\r\n",
        "      axes[count].legend()\r\n",
        "    count+=1\r\n",
        "\r\n",
        "# plot_signals(x_train[101], title=\"Scaled Signals - x_train[0]\")\r\n",
        "# plot_signals(x_train_unscaled[101], title=\"Unscaled Signals - x_train[0]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG59YSmdUE55"
      },
      "source": [
        "#### Take a subset of data for testing autoencoder configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je4mpLs1xysD"
      },
      "source": [
        "# Take a subset of the data - only one feature, every 5 timepoints, to test different AE configurations.\r\n",
        "x_train_1f = x_train[::5,0]\r\n",
        "\r\n",
        "# Fully Connected Autoencoder requires a flatter 2D structure where the timepoints & features are both in the columns.\r\n",
        "x_train_1f_2D = x_train_1f.reshape((int(len(x_train_1f)/500), 500)) \r\n",
        "print(\"2D subset of data, shape :\",  x_train_1f_2D)\r\n",
        "\r\n",
        "# Convolutional layer requires the shape (n_samples, n_timepoints, n_features)\r\n",
        "x_train_1f = x_train_1f.reshape((int(len(x_train_1f)/500), 500, 1)) # We're only taking one channel for now, so last dimension is 1.\r\n",
        "print(\"3D subset of data, shape :\", x_train_1f.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PWDBiuWVX8"
      },
      "source": [
        "## Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YonvAGL9lmn6"
      },
      "source": [
        "\r\n",
        "#### LSTM Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWaAMdQtx5FA"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "def autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(X.shape[1], X.shape[2]))\r\n",
        "    L1 = LSTM(18, activation='tanh', return_sequences=True, \r\n",
        "              kernel_regularizer=regularizers.l2(0.00))(inputs)\r\n",
        "    L2 = LSTM(450, activation='tanh', return_sequences=False)(L1)\r\n",
        "    L3 = RepeatVector(X.shape[1])(L2)\r\n",
        "    L4 = LSTM(450, activation='tanh', return_sequences=True)(L3)\r\n",
        "    L5 = LSTM(18, activation='tanh', return_sequences=True)(L4)\r\n",
        "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \r\n",
        "    model = Model(inputs=inputs, outputs=output)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTUoSyKlypi7"
      },
      "source": [
        "#### Fully Connected Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okqWORve_W0b"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "def fc_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(500,))\r\n",
        "    e1 = Dense(225, activation='relu')(inputs)\r\n",
        "    e2 = Dense(150, activation='relu')(e1)\r\n",
        "    e3 = Dense(70, activation='relu')(e2)  \r\n",
        "    encoded = Dense(70, activation='relu')(e3)\r\n",
        "    d1 = Dense(70, activation='relu')(encoded)  \r\n",
        "    d2 = Dense(150, activation='relu')(d1)\r\n",
        "    d3 = Dense(225, activation='relu')(d2)\r\n",
        "    decoded = Dense(X.shape[1], activation='sigmoid')(d3)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=decoded)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJjIzolOx8xV"
      },
      "source": [
        "# create the autoencoder model\r\n",
        "model = fc_autoencoder_model(x_train_1f)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUsOKCEyxv1"
      },
      "source": [
        "#### Convolutional Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH5fiyjlXLqh"
      },
      "source": [
        "##### Code for small tests on output shapes of different layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtK53tcGzfL3"
      },
      "source": [
        "### Testing only, not a working model.\r\n",
        "inputs = Input(shape=(500,1))\r\n",
        "t1 =  Conv1D(10, 9, padding='same')(inputs)\r\n",
        "t2 = MaxPooling1D(2)(t1)\r\n",
        "# t3 = Flatten()(t2)\r\n",
        "\r\n",
        "t4 =  Conv1DTranspose(1, 9, padding='same')(t2)\r\n",
        "\r\n",
        "t5 = Conv1D(4,5, padding='same')(t2)\r\n",
        "\r\n",
        "t6 = MaxPooling1D(3)(t4)\r\n",
        "\r\n",
        "test = Model(inputs=inputs, outputs=t2)\r\n",
        "test.output_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltaV2CiYXS5J"
      },
      "source": [
        "##### Define a convolutional autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4dPRBsVy2Ot"
      },
      "source": [
        "# Testing around with adding or removing certain layers, no huge difference in performance vs. just using a simpler architecture outlined a few cells down.\r\n",
        "def conv_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(X.shape[1],1))\r\n",
        "    e1 = Conv1D(4, 9, activation='relu', padding='same')(inputs)\r\n",
        "    e2 = MaxPooling1D(2)(e1)\r\n",
        "    e3 = Conv1D(2,5, activation = 'relu', padding='same')(e2)\r\n",
        "    e4 = MaxPooling1D(5)(e3)\r\n",
        "    # e5 = Flatten()(e4)\r\n",
        "    # encoded = Dense(100, activation='relu')(e5)\r\n",
        "    # d1 = Dense(100, activation='relu')(encoded)\r\n",
        "    # d2 = Reshape((50,2))(d1)\r\n",
        "    d3 = UpSampling1D(5)(e4)\r\n",
        "    d4 = Conv1DTranspose(4, 5, activation='relu', padding='same')(d3)\r\n",
        "    d5 = UpSampling1D(2)(d4)\r\n",
        "    d6 = Conv1DTranspose(1, 9, activation='relu', padding='same')(d5)\r\n",
        "    # decoded = Dense(500, activation='sigmoid')(d6)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=d6)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge5xe61zy4AL"
      },
      "source": [
        "# create the autoencoder model\r\n",
        "# model = conv_autoencoder_model(x_train_1f)\r\n",
        "model = conv_autoencoder_model(x_train_filtered[:,:,0])\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sonbmyiXa2l"
      },
      "source": [
        "##### Define a simple convolutional autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgn_4J2ARQCW"
      },
      "source": [
        "# define the autoencoder network model (Keras)\r\n",
        "def simple_conv_autoencoder_model(X):\r\n",
        "    inputs = Input(shape=(500,1))\r\n",
        "    e1 = Conv1D(4, 9, activation='relu', padding='same')(inputs)\r\n",
        "    e2 = MaxPooling1D(10)(e1)\r\n",
        "    d5 = UpSampling1D(10)(e2)\r\n",
        "    d6 = Conv1DTranspose(1, 9, activation='sigmoid', padding='same')(d5)\r\n",
        "\r\n",
        "    model = Model(inputs=inputs, outputs=d6)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJlY5HJdR6hf"
      },
      "source": [
        "# create the simple autoencoder model\r\n",
        "model = simple_conv_autoencoder_model(x_train_1f)\r\n",
        "model.compile(optimizer='adam', loss='mae')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUwCCZ0ZXxdR"
      },
      "source": [
        "##### Fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzzXJTvNyAn0"
      },
      "source": [
        "# fit the model to the data\r\n",
        "nb_epochs = 20\r\n",
        "batch_size = 20\r\n",
        "# history = model.fit(x_train_1f, x_train_1f, epochs=nb_epochs,\r\n",
        "#                     batch_size=batch_size,\r\n",
        "#                     # validation_split=0.05\r\n",
        "#                     ).history\r\n",
        "## Try to train the model on 1 channel but all timepoints after band-pass filter\r\n",
        "history = model.fit(x_train_filtered[:,:,0], x_train_filtered[:,:,0], epochs=nb_epochs,\r\n",
        "                    batch_size=batch_size,\r\n",
        "                    # validation_split=0.05\r\n",
        "                    ).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGDjFwgX8pZ"
      },
      "source": [
        "##### Plot the reconstruction for individual samples.\r\n",
        "Temporary code for assessing the models on the subset of data (one channel and only 20% of timepoints)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbHkzSliPCa"
      },
      "source": [
        "# Plot samples.\r\n",
        "x_pred = model.predict(x_train_filtered[:,:,3])\r\n",
        "\r\n",
        "# Plot actuals vs. prediction for one column across all rows\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.scatter(x_train_filtered[:,0,3], x_pred[:,0,0],alpha=0.2)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Plot actuals vs. predictions for one row across all columns\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_train_filtered[0,:,3], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[0,:,0], label= 'predicted',alpha=0.7)\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "# Plot same as above for a different row\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_train_filtered[1039,:,3], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[1039,:,0], label= 'predicted',alpha=0.7)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbI4PWeyKxQT"
      },
      "source": [
        "# Plot samples.\r\n",
        "x_pred = model.predict(x_train_1f)\r\n",
        "\r\n",
        "# Plot actuals vs. prediction for one column across all rows\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.scatter(x_train_1f[:,0], x_pred[:,0],alpha=0.2)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Plot actuals vs. predictions for one row across all columns\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_train_1f[0,:], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[0,:], label= 'predicted',alpha=0.7)\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "# Plot same as above for a different row\r\n",
        "plt.figure(figsize=(14,8))\r\n",
        "plt.plot(x_train_1f[1039,:], label='actual', alpha=0.7)\r\n",
        "plt.plot(x_pred[1039,:], label= 'predicted',alpha=0.7)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCfIG-Na0Rim"
      },
      "source": [
        "\r\n",
        "## Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOMojTzGYVH2"
      },
      "source": [
        "# plot the training losses\r\n",
        "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\r\n",
        "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\r\n",
        "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\r\n",
        "ax.set_title('Model loss', fontsize=16)\r\n",
        "ax.set_ylabel('Loss (mae)')\r\n",
        "ax.set_xlabel('Epoch')\r\n",
        "ax.legend(loc='upper right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AvXQtdVLPT"
      },
      "source": [
        "# Get the predicted values for the training set.\r\n",
        "X_pred_3D = model.predict(x_train)\r\n",
        "X_pred = X_pred_3D.reshape(X_pred_3D.shape[0]*X_pred_3D.shape[1], X_pred_3D.shape[2])\r\n",
        "# X_pred = pd.DataFrame(X_pred, columns=train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIl53WipVnBA"
      },
      "source": [
        "##### Plot distribution of the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERj4N100Maz"
      },
      "source": [
        "# Plot the distribution of the loss\r\n",
        "x_train_reshaped = x_train.reshape(x_train.shape[0]*x_train.shape[1], x_train.shape[2])\r\n",
        "loss_mae = np.mean(np.abs(X_pred-x_train_reshaped), axis = 1)\r\n",
        "plt.figure(figsize=(16,9), dpi=80)\r\n",
        "plt.title('Loss Distribution', fontsize=16)\r\n",
        "sns.distplot(loss_mae, bins = 20, kde= True, color = 'blue');\r\n",
        "plt.xlim([0.0,.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uQFT7_1Vtt1"
      },
      "source": [
        "##### Evaluate total re-construction for one sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er4v0JqLR-0M"
      },
      "source": [
        "cols = ['F3', 'F4', 'FC3', 'FC4', 'C3', 'Cz', 'C4', 'CP3', 'CP4']\r\n",
        "\r\n",
        "def evaluate_prediction(actuals, pred, ind, rescale=False):\r\n",
        "  \"\"\" Function to plot predictions vs. the actuals for one sample.\r\n",
        "  input:\r\n",
        "    actuals   3D array (n_samplesx2500x9) - Original scaled signals.\r\n",
        "    pred      the predicted values corresponding to the actuals.\r\n",
        "    ind     The row number of the sample (2500x9) that you want to compare.\r\n",
        "    rescale   If set to true then the data is first converted back to the original scale for comparison.\r\n",
        "  \r\n",
        "  returns:\r\n",
        "    nothing\r\n",
        "\r\n",
        "  prints plots.\r\n",
        "  \"\"\"\r\n",
        "  if rescale:\r\n",
        "    sample_actual = scaler.inverse_transform(actuals[ind]) # Rescale to the original scale.\r\n",
        "    sample_pred = scaler.inverse_transform(pred[ind]) \r\n",
        "  else:\r\n",
        "    sample_actual = actuals[ind]    # Get the relevant sample.\r\n",
        "    sample_pred = pred[ind]\r\n",
        "\r\n",
        "  mae_by_channel = np.mean(np.abs(sample_pred - sample_actual), axis=0) # Get the Mean Absolute Error for each channel for this sample\r\n",
        "  sample_mae = np.mean(mae_by_channel) # Get the total MAE for the sample by taking the average across the 9 channels\r\n",
        "  print(\"Sample\", ind, \"\\n   Total Mean Absolute Error:\", round(sample_mae, 8))\r\n",
        "  print(\"Mean Absolute Error by Channel:\")\r\n",
        "  for col, error in zip(cols, mae_by_channel):\r\n",
        "    print(col, \": \", round(error,8)) \r\n",
        "\r\n",
        "  fig, axes = plt.subplots(3,3, figsize=(9,9))\r\n",
        "  axes=axes.ravel()\r\n",
        "\r\n",
        "  for i in range(9):\r\n",
        "    axes[i].plot(sample_actual[:,i], label= \"Actual\")\r\n",
        "    axes[i].plot(sample_pred[:,i], label=\"Predicted\")\r\n",
        "    axes[i].title.set_text(cols[i] + str(round(mae_by_channel[i], 3)))\r\n",
        "  \r\n",
        "  plt.legend()\r\n",
        "  fig.suptitle(\"Predictions vs. Actuals - Sample \" + str(ind),size=16)\r\n",
        "  fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYRpNQjWF-u"
      },
      "source": [
        "evaluate_prediction(x_train, X_pred_3D, ind=1, rescale=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ehGjGxUV4Vf"
      },
      "source": [
        "##### Plot the loss distribution of the test set (code from tutorial, needs to be edited)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk2FB1JADjIE"
      },
      "source": [
        "# plot the loss distribution of the test set\r\n",
        "X_pred = model.predict(x_test)\r\n",
        "X_pred = X_pred.reshape(X_pred.shape[0]*X_pred.shape[1], X_pred.shape[2])\r\n",
        "\r\n",
        "x_test_reshaped = x_test.reshape(x_test.shape[0]*x_test.shape[1], x_test.shape[2])\r\n",
        "fig, axes = plt.subplots(9,1, figsize=(18,9))\r\n",
        "# Plot the loss distribution for each channel individually\r\n",
        "for i in range(x_test_reshaped.shape[1]):\r\n",
        "  loss_mae = np.abs(X_pred[:,i]-x_test_reshaped[:,i])\r\n",
        "  sns.distplot(loss_mae, bins = 100, kde= True, color = 'blue', ax=axes[i]);\r\n",
        "  axes[i].axis(xmin=0.0,xmax=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVIfqljx0lwt"
      },
      "source": [
        "# save all model information, including weights, in h5 format\r\n",
        "model.save(\"/content/drive/MyDrive/ml2-eeg-biometrics/model1_rw.h5\")\r\n",
        "print(\"Model saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0h0mPobmb50"
      },
      "source": [
        "#Convolutional Auto Encoder\r\n",
        "This is a custom implementation, architecture and parameters need to be optimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2xNo6s6m0dV"
      },
      "source": [
        "# The architecture is inspired by the following thread \r\n",
        "# https://stackoverflow.com/questions/49290895/how-to-implement-a-1d-convolutional-auto-encoder-in-keras-for-vector-data\r\n",
        "\r\n",
        "class Autoencoder():\r\n",
        "    def __init__(self):\r\n",
        "        self.time_steps = 2500\r\n",
        "        self.channels = 9\r\n",
        "        self.input_sigmoid = (self.time_steps, self.channels)\r\n",
        "        \r\n",
        "        optimizer = Adam(lr=0.001)\r\n",
        "        \r\n",
        "        self.autoencoder_model = self.build_model()\r\n",
        "        self.autoencoder_model.compile(loss='mse', optimizer=optimizer)\r\n",
        "        self.autoencoder_model.summary()\r\n",
        "    \r\n",
        "    def build_model(self):\r\n",
        "        input_layer = Input(shape=self.input_sigmoid)\r\n",
        "        \r\n",
        "        # encoder\r\n",
        "        x = Conv1D(filters=64, kernel_size=7, activation='relu', padding='same',dilation_rate=2)(input_layer) # When using this layer as the first layer in a model, provide an input_shape argument \r\n",
        "                                                                                                             # (tuple of integers or None, e.g. (10, 128) for sequences of 10 vectors of 128-dimensional vectors, \r\n",
        "                                                                                                             # or (None, 128) for variable-length sequences of 128-dimensional vectors.\r\n",
        "        x1 = MaxPooling1D(pool_size=2)(x) # Downsamples the input representation by taking the maximum value over the window defined by pool_size. The window is shifted by strides. \r\n",
        "                                          # The resulting output when using \"valid\" padding option has a shape of: output_shape = (input_shape - pool_size + 1) / strides)\r\n",
        "        x2 = Conv1D(filters=32,kernel_size=7, activation='relu', padding='same',dilation_rate=2)(x1)\r\n",
        "        x3 = MaxPooling1D(pool_size=2)(x2)\r\n",
        "        x4 = AveragePooling1D()(x3)\r\n",
        "        flat = Flatten()(x4)\r\n",
        "        encoded = Dense(units=70)(flat)\r\n",
        "\r\n",
        "        # decoder\r\n",
        "        d1 = Dense(units=36)(encoded) # Densely-connected NN layer.\r\n",
        "        d2 = Reshape((18,2))(d1) #Layer that reshapes inputs into the given shape.\r\n",
        "        d3 = Conv1D(filters=32,kernel_size=1,strides=1, activation='relu', padding='same')(d2)\r\n",
        "        d4 = UpSampling1D(size=2)(d3) #Repeats each temporal step size times along the time axis.\r\n",
        "        d5 = Conv1D(filters=32,kernel_size=1,strides=1, activation='relu', padding='same')(d4)\r\n",
        "        d6 = UpSampling1D(size=2)(d5)\r\n",
        "        d7 = UpSampling1D(size=2)(d6)\r\n",
        "        decoded = Conv1D(filters=1,kernel_size=1,strides=1, activation='sigmoid', padding='same')(d7)\r\n",
        "        \r\n",
        "        model = Model(input_layer, x)\r\n",
        "        model.output_shape\r\n",
        "        return model\r\n",
        "                      \r\n",
        "    def train_model(self, x_train, y_train, x_val, y_val, epochs, batch_size=20):\r\n",
        "        early_stopping = EarlyStopping(monitor='val_loss',\r\n",
        "                                       min_delta=0,\r\n",
        "                                       patience=5,\r\n",
        "                                       verbose=1, \r\n",
        "                                       mode='auto')\r\n",
        "        history = self.autoencoder_model.fit(x_train, y_train,\r\n",
        "                                             batch_size=batch_size,\r\n",
        "                                             epochs=epochs,\r\n",
        "                                             validation_data=(x_val, y_val),\r\n",
        "                                             callbacks=[early_stopping])\r\n",
        "        plt.plot(history.history['loss'])\r\n",
        "        plt.plot(history.history['val_loss'])\r\n",
        "        plt.title('Model loss')\r\n",
        "        plt.ylabel('Loss')\r\n",
        "        plt.xlabel('Epoch')\r\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "        plt.show()\r\n",
        "    \r\n",
        "    def eval_model(self, x_test):\r\n",
        "        preds = self.autoencoder_model.predict(x_test)\r\n",
        "        return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epWwkAYHpk0m"
      },
      "source": [
        "model = Autoencoder()\r\n",
        "model.train_model(x_train, y_train, x_valid, y_valid, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5_hkqH0ojzI"
      },
      "source": [
        "# save all model information, including weights, in h5 format\r\n",
        "model.save(\"/content/drive/MyDrive/ml2-eeg-biometrics/model2_1dconv_autoencoder.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-_9EyL8dscb"
      },
      "source": [
        "##  CNN from tutorial - unedited code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnTNfQ69drU_"
      },
      "source": [
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from numpy import dstack\r\n",
        "from pandas import read_csv\r\n",
        "from matplotlib import pyplot\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lob7iKO-d1NH"
      },
      "source": [
        "# cnn model\r\n",
        "\r\n",
        "# fit and evaluate a model\r\n",
        "def evaluate_model(trainX, trainy, testX, testy, n_filters):\r\n",
        "  verbose, epochs, batch_size = 0, 10, 32\r\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\r\n",
        "  model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu'))\r\n",
        "  model.add(Dropout(0.5))\r\n",
        "  model.add(MaxPooling1D(pool_size=2))\r\n",
        "  model.add(Flatten())\r\n",
        "  model.add(Dense(100, activation='relu'))\r\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "  # fit network\r\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\r\n",
        "  # evaluate model\r\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\r\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK4gsDmgd5le"
      },
      "source": [
        "# summarize scores\r\n",
        "def summarize_results(scores, params):\r\n",
        "  print(scores, params)\r\n",
        "  # summarize mean and standard deviation\r\n",
        "  for i in range(len(scores)):\r\n",
        "    m, s = mean(scores[i]), std(scores[i])\r\n",
        "    print('Param=%d: %.3f%% (+/-%.3f)' % (params[i], m, s))\r\n",
        "  # boxplot of scores\r\n",
        "  pyplot.boxplot(scores, labels=params)\r\n",
        "  pyplot.savefig('exp_cnn_filters.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rv7wgcnd8jn"
      },
      "source": [
        "# run an experiment\r\n",
        "def run_experiment(params, repeats=10):\r\n",
        "  # test each parameter\r\n",
        "  all_scores = list()\r\n",
        "  for p in params:\r\n",
        "    # repeat experiment\r\n",
        "    scores = list()\r\n",
        "    for r in range(repeats):\r\n",
        "      score = evaluate_model(x_train, y_train, x_test, y_test, p)\r\n",
        "      score = score * 100.0\r\n",
        "      print('>p=%d #%d: %.3f' % (p, r+1, score))\r\n",
        "      scores.append(score)\r\n",
        "    all_scores.append(scores)\r\n",
        "  # summarize results\r\n",
        "  summarize_results(all_scores, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqlXCQ8DeBnQ"
      },
      "source": [
        "# run the experiment\r\n",
        "n_params = [8, 16, 32, 64, 128, 256]\r\n",
        "run_experiment(n_params)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}